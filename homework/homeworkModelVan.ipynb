{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('PY379': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fd79c5f311e48de877911d8cf1d56e7b724990a605ba536e714d512526dbebf5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "\n",
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# # import os\n",
    "# # print(os.listdir(\".\"))\n",
    "# # Load dataset\n",
    "train_df = pd.read_csv('./Training_Data.csv') # 800 筆\n",
    "\n",
    "test_df = pd.read_csv('./Testing_Data.csv') # 800 筆\n",
    "# # Any results you write to the current directory are saved as output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_names = df.columns\n",
    "# for c in col_names:\n",
    "# \tdf[c] = df[c].replace(\"?\", np.NaN)\n",
    "\n",
    "# df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['STATUS', 'CHIPNAME', 'LAYER', 'REMAIN_LAYER_SEQ', 'PRIORITY',\n       'LOT_TYPE', 'WIP_QTY', 'IS_MAIN_ROUTE', 'remainDT'],\n      dtype='object')\nIndex(['STATUS', 'CHIPNAME', 'LAYER', 'REMAIN_LAYER_SEQ', 'PRIORITY',\n       'LOT_TYPE', 'WIP_QTY', 'IS_MAIN_ROUTE', 'remainDT'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# df.replace(['Divorced', 'Married-AF-spouse', 'Married-civ-spouse', 'Married-spouse-absent','Never-married','Separated','Widowed'],\n",
    "#              ['divorced','married','married','married','not married','not married','not married'], inplace = True)\n",
    "\n",
    " #指定參數 axis = 0 表示要刪除觀測值（row），指定參數 axis = 1 表示要刪除欄位（column）。\n",
    "train_df = train_df.drop(['LOT_ID', 'DATA_DATE','IDX','0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n",
    "       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n",
    "       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n",
    "       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n",
    "       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n",
    "       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n",
    "       'TV', 'UG','ACTUAL_WP_OUT', 'WS_DATE','out', 'in', 'cytime','WIPDT'], axis=1)  \n",
    "print(train_df.columns)  \n",
    "test_df = test_df.drop(['LOT_ID', 'DATA_DATE','IDX','0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n",
    "       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n",
    "       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n",
    "       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n",
    "       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n",
    "       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n",
    "       'TV', 'UG','ACTUAL_WP_OUT', 'WS_DATE','out', 'in', 'cytime','WIPDT'], axis=1)  \n",
    "print(test_df.columns)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STATUS              0\nCHIPNAME            0\nLAYER               0\nREMAIN_LAYER_SEQ    0\nPRIORITY            0\nLOT_TYPE            0\nWIP_QTY             0\nIS_MAIN_ROUTE       0\nremainDT            0\ndtype: int64\nSTATUS              0\nCHIPNAME            0\nLAYER               0\nREMAIN_LAYER_SEQ    0\nPRIORITY            0\nLOT_TYPE            0\nWIP_QTY             0\nIS_MAIN_ROUTE       0\nremainDT            0\ndtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 刪除null值\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())\n"
   ]
  },
  {
   "source": [
    "# 原始資料是有序離散值的話 => Label Encoding\n",
    "# 原始資料是無序離散值的話 => One Hot Encoding (Dummies)\n",
    "\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "category_col =['STATUS', 'CHIPNAME', 'LAYER','LOT_TYPE'] \n",
    "#從上面的資料可以看到country那欄皆為字串， 大部分的模型都是基於數學運算，字串無法套入數學模型進行運算，在此先對其進行Label encoding編碼，我們從 sklearn library中導入 LabelEncoder class，對第一行資料進行fit及transform並取代之\n",
    "for col in category_col:\n",
    "    train_df[col] = labelEncoder.fit_transform(train_df[col])\n",
    "for col in category_col:\n",
    "    test_df[col] = labelEncoder.fit_transform(test_df[col])\n",
    "#One hot encoding  使用Pandas進行   \n",
    "# category_col_1 =[ 'STATUS', 'CHIPNAME', 'LAYER', 'REMAIN_LAYER_SEQ', 'PRIORITY',\n",
    "#        'LOT_TYPE', 'IS_MAIN_ROUTE']            \n",
    "train_df = pd.get_dummies(train_df) #, columns=category_col_1, drop_first=True\n",
    "test_df = pd.get_dummies(test_df) #, columns=category_col_1, drop_first=True\n",
    "# onehotencoder = OneHotEncoder()\n",
    "# data_str_ohe=onehotencoder.fit_transform(train_df).toarray()\n",
    "# train_df = pd.DataFrame(data_str_ohe)\n",
    "\n",
    "\n",
    "\n",
    "##unknown Attribute is removed and income class label is appended in the end\n",
    "# dataframe=df_2.drop('fnlwgt',1)\n",
    "# dataframe =dataframe[[c for c in dataframe if c not in ['income']] + ['income']]\n",
    "# dataframe.head(20)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 97,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STATUS              0\nCHIPNAME            0\nLAYER               0\nREMAIN_LAYER_SEQ    0\nPRIORITY            0\nLOT_TYPE            0\nWIP_QTY             0\nIS_MAIN_ROUTE       0\nremainDT            0\ndtype: int64\nSTATUS              0\nCHIPNAME            0\nLAYER               0\nREMAIN_LAYER_SEQ    0\nPRIORITY            0\nLOT_TYPE            0\nWIP_QTY             0\nIS_MAIN_ROUTE       0\nremainDT            0\ndtype: int64\n(47556, 9)\n(1005, 9)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.iloc[:, :8].values\n",
    "y_train = train_df.iloc[:, 8].values\n",
    "\n",
    "X_test = test_df.iloc[:, :8].values\n",
    "y_test = test_df.iloc[:, 8].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(47556, 8)\n(47556,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.1150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #引入分類器\n",
    "rf = RandomForestClassifier(criterion='gini', \n",
    "                             n_estimators=1000,\n",
    "                             min_samples_split=12,\n",
    "                             min_samples_leaf=1,\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1) \n",
    "\n",
    "rf.fit(train_df.iloc[:, :8], train_df.iloc[:, 8])\n",
    "print(\"%.4f\" % rf.oob_score_)\n",
    "\n",
    "# submit = pd.read_csv('./gender_submission.csv')\n",
    "# rf_res =  rf.predict(dataTest)\n",
    "# submit['Survived'] = rf_res\n",
    "# submit['Survived'] = submit['Survived'].astype(int)\n",
    "# submit.to_csv('submit.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 115
    }
   ],
   "source": [
    "#rfc=RandomForestClassifier(n_estimators=5,random_state=0) #隨機森林分類器 選擇要用多少棵決策樹 =5，隨機種子要固定下來，比較能夠確定優化結果是否是真的增加準確度\n",
    "rfc = RandomForestClassifier(n_estimators=5)\n",
    "\n",
    "rfc.fit(X_train,y_train)\n",
    "y_predict=rfc.predict(X_test)\n",
    "rfc.score(X_test,y_test) #output 預測成功率 0.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.11840796019900497"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "#====================================================\n",
    "#====================================================\n",
    "rfc=RandomForestClassifier(n_estimators=100) \n",
    "#隨機森林分類器 選擇要用多少棵決策樹 =100==> 決策數的樹木\n",
    "rfc.fit(X_train,y_train)\n",
    "y_predict=rfc.predict(X_test)\n",
    "rfc.score(X_test,y_test) #output 預測成功率 0.95(成功率有提升)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "rfc=RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=50,min_samples_leaf=10) \n",
    "#隨機森林分類器 選擇要用多少棵決策樹 =100==> 決策數的樹木\n",
    "#n_jobs =-1 :多核心有多少就用多少核心一起運算\n",
    "#min_sample_leaf: 修剪樹枝，生長完後最後做樹枝的修剪 最少包含10個資訊量\n",
    "\n",
    "rfc.fit(X_train,y_train)\n",
    "y_predict=rfc.predict(X_test)\n",
    "rfc.score(X_test,y_test) #output 預測成功率 0.93(成功率有提升)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[51. 22.  3. ... 35. 45. 41.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-d236535fac22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFS_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mFS_2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFS_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\PY379\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 667\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\PY379\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    696\u001b[0m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001b[0;32m    697\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m                                 force_all_finite='allow-nan')\n\u001b[0m\u001b[0;32m    699\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\PY379\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;34mf\"requires y to be passed, but the target y is None.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m                 )\n\u001b[1;32m--> 420\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\PY379\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\PY379\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[51. 22.  3. ... 35. 45. 41.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "#標準化數據-StandardScaler\n",
    "sc = StandardScaler()\n",
    "import sklearn.preprocessing as sp\n",
    "\n",
    "FS_1=sp.StandardScaler().fit(X_train)\n",
    "X_train=FS_1.transform(X_train)\n",
    "\n",
    "FS_2=sp.StandardScaler().fit(y_train)\n",
    "y_train=FS_2.transform(y_train)\n",
    "\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Sequential()\n",
    "\n",
    "# classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 88))\n",
    "# classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "# classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "# classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "                   \n",
    "# classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)\n",
    "\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(47983, 9)"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立模型\n",
    "print(\"\\n[Info] 建立模型\")  \n",
    "\n",
    "\n",
    "classifier = Sequential()\n",
    "# 輸入層\n",
    "classifier.add(Dense(units=3, input_dim=1, kernel_initializer='uniform', activation='relu'))\n",
    "#model.add(Dense(units=3, input_dim=9, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# 隱藏層\n",
    "classifier.add(Dense(units=5, kernel_initializer='uniform', activation='relu'))\n",
    "#model.add(Dense(units=40, kernel_initializer='uniform', activation='relu'))\n",
    "#model.add(Dense(units=30, kernel_initializer='uniform', activation='relu'))\n",
    "\n",
    "# 輸出層\n",
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "#print(\"\\n[Info] Show model summary...\")  \n",
    "classifier.summary()\n",
    "\n",
    "# 進行訓練\n",
    "print(\"\\n[Info] 訓練中...\")  \n",
    "classifier.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "train_history = classifier.fit(x=X_train, y=y_train, validation_split=0.1, epochs=3, batch_size=30, verbose=2)  \n",
    "\n",
    "#train_history = model.fit(x=train_features, y=train_labels, validation_split=0.1, epochs=50, batch_size=30, verbose=2)  \n",
    "\n",
    "#val_df = pd.read_csv(r'D:\\Project\\MyPython\\titanic\\data\\val.csv')  \n",
    "#val_features, val_labels = data_preparer.preprocess(val_df)\n",
    "#train_history = model.fit(x=train_features, y=train_labels, validation_data=(val_features, val_labels), epochs=50, batch_size=30, verbose=2)  \n",
    "#print(\"\\n[Info] 訓練成效 (文字)\")  \n",
    "#print(train_history.history)\n",
    "\n",
    "\n",
    "# # 評估模型\n",
    "# #loss_val, acc_val, mse_val = model.evaluate(val_features, val_labels)\n",
    "# #print(f\"\\n評估模型 : Loss is {loss_val},\\nAccuracy is {acc_val * 100},\\nMSE is {mse_val}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 顯示結果\n",
    "# #print(\"\\n[Info] 訓練成效 (圖表)\")\n",
    "# import loss_plot\n",
    "# import acc_plot \n",
    "# loss_plot.draw(train_history)\n",
    "# acc_plot.draw(train_history)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sys.exit(\"stop\")\n",
    "\n",
    "# # 預測\n",
    "# Jack = pd.Series([0, 'Jack', 3, 'male', 23, 1, 0, 5.0, 'S'])  \n",
    "# Rose = pd.Series([1, 'Rose', 1, 'female', 28, 1, 0, 100.0, 'S'])  \n",
    "# JR_df = pd.DataFrame([list(Jack), list(Rose)], columns=['Survived','Name', 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'])  \n",
    "\n",
    "# all_df = JR_df\n",
    "# #all_df = pd.concat([train_df, JR_df]) # 將 \"待預測項目\" 加入母體\n",
    "\n",
    "# print(\"\\n[Info] 預測中...\")\n",
    "# features, labels = data_preparer.preprocess(all_df)\n",
    "# all_probability = model.predict(features)  \n",
    "# all_df.insert(len(all_df.columns), 'probability', all_probability * 100) # 加入生存機率 欄位\n",
    "# print(\"\\n[Info] 預測結果 (傑克 & 蘿絲):\\n%s\\n\" % (all_df[-2:]))\n",
    "# #print(\"\\n[Info] 預測結果 (所有乘客):\\n%s\\n\" % (all_df))\n",
    "\n",
    "\n",
    "# #test_df = pd.read_csv(r'D:\\Project\\MyPython\\titanic\\data\\test.csv')  \n",
    "# #test_features, test_labels = data_preparer.preprocess(test_df)"
   ]
  },
  {
   "source": [
    "Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"\\nAccuracy ========== >>>  85.41%\\n\")\n",
    "\n",
    "#classification report\n",
    "print (\"\\nClassification Report\\n\")\n",
    "print (classification_report(y_test, y_pred))\n",
    "\n",
    "conf_arr =confusion_matrix(y_test, y_pred)\n",
    "norm_conf = []\n",
    "for i in conf_arr:\n",
    "    a = 0\n",
    "    tmp_arr = []\n",
    "    a = sum(i, 0)\n",
    "    for j in i:\n",
    "        tmp_arr.append(float(j)/float(a))\n",
    "    norm_conf.append(tmp_arr)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.clf()\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.set_aspect(1)\n",
    "res = ax.imshow(np.array(norm_conf), cmap=plt.cm.jet, \n",
    "                interpolation='nearest')\n",
    "\n",
    "width, height = conf_arr.shape\n",
    "\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        ax.annotate(str(conf_arr[x][y]), xy=(y, x), \n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center')\n",
    "## confusion matrix\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(range(width), ['positive','negative'])\n",
    "plt.yticks(range(height), ['positive','negative'])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "plt.title('Receiver Operating Characteristic Decision Tree')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "##gini coefficient\n",
    "Gini_coefficient=2*roc_auc - 1\n",
    "print(\"Gini_coefficient from the ROC curve is \\n\",Gini_coefficient)"
   ]
  }
 ]
}