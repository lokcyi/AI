{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('PY379': conda)",
   "metadata": {
    "interpreter": {
     "hash": "fd79c5f311e48de877911d8cf1d56e7b724990a605ba536e714d512526dbebf5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['DATA_DATE', 'IDX', 'LOT_ID', 'STATUS', 'CHIPNAME', 'LAYER',\n       'REMAIN_LAYER_SEQ', 'PRIORITY', 'LOT_TYPE', 'WIP_QTY', 'WS_DATE',\n       'IS_MAIN_ROUTE', '0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n       'TV', 'UG', 'ACTUAL_WP_OUT', 'out', 'in', 'cytime', 'remainDT',\n       'WIPDT'],\n      dtype='object')\nIndex(['DATA_DATE', 'IDX', 'LOT_ID', 'STATUS', 'CHIPNAME', 'LAYER',\n       'REMAIN_LAYER_SEQ', 'PRIORITY', 'LOT_TYPE', 'WIP_QTY', 'WS_DATE',\n       'IS_MAIN_ROUTE', '0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n       'TV', 'UG', 'ACTUAL_WP_OUT', 'out', 'in', 'cytime', 'remainDT',\n       'WIPDT'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "#import data_preparer\n",
    "train_df = pd.read_csv('./Training_Data.csv') # 800 筆\n",
    "test_df = pd.read_csv('./Testing_Data.csv') # 800 筆\n",
    "print(train_df.columns)\n",
    "print(test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0db42f1a8673>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m train_df = train_df.drop(['IDX','0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n\u001b[0m\u001b[0;32m      2\u001b[0m        \u001b[1;34m'1T'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'1U'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'1V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2E'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2F'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2T'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2U'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m        \u001b[1;34m'2V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3E'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3G'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3K'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3S'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3T'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3U'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m        \u001b[1;34m'3V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4E'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4T'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4U'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4V'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5E'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5I'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m'5M'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5S'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6P'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7I'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'8D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'8I'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = train_df.drop(['IDX','0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n",
    "       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n",
    "       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n",
    "       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n",
    "       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n",
    "       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n",
    "       'TV', 'UG'], axis=1)  \n",
    "print(train_df.columns)  \n",
    "test_df = test_df.drop(['IDX','0I', '1C', '1D', '1F', '1G', '1I', '1M', '1N', '1P',\n",
    "       '1T', '1U', '1V', '2D', '2E', '2F', '2I', '2M', '2N', '2P', '2T', '2U',\n",
    "       '2V', '3D', '3E', '3G', '3I', '3K', '3M', '3N', '3P', '3S', '3T', '3U',\n",
    "       '3V', '4D', '4E', '4I', '4M', '4N', '4T', '4U', '4V', '5D', '5E', '5I',\n",
    "       '5M', '5N', '5P', '5S', '6D', '6I', '6N', '6P', '7D', '7I', '8D', '8I',\n",
    "       '9D', 'DO', 'ES', 'GN', 'HR', 'LI', 'PO', 'PV', 'SP', 'SV', 'TM', 'TU',\n",
    "       'TV', 'UG'], axis=1)  \n",
    "print(test_df.columns)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(train_df.info())\n",
    "\n",
    "print(test_df.shape)\n",
    "print(test_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Columns like  \n",
    "#Continuous Columns like  \n",
    "#Requirement : I need count of output date\n",
    "\n",
    "# print(pd.crosstab(train_df.out,columns='WIPDT',margins=True))\n",
    "# print(pd.crosstab(test_df.out,columns='WIPDT',margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirement: All Numerical columns basic statistics.\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirement: All Numerical columns basic statistics.\n",
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sweetviz as sv\n",
    "pairwise_analysis='off' #相關性和其他型別的資料關聯可能需要花費較長時間。如果超過了某個閾值，就需要設定這個引數為on或者off，以判斷是否需要分析資料相關性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_train = sv.analyze([train_df, 'train']) # 'train'是指會給這個資料集命名為train\n",
    "report_train.show_html(filepath='Basic_train_report.html') # 儲存為html的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_config = sv.FeatureConfig(skip=['DATA_DATE','ACTUAL_WP_OUT','WS_DATE'],  # 要忽略哪個特徵\n",
    "                                  force_cat=[ 'LOT_ID', 'STATUS', 'CHIPNAME', 'LAYER','PRIORITY', 'LOT_TYPE',  'WS_DATE','IS_MAIN_ROUTE'], # Categorical特徵\n",
    "                                  force_num=['WIP_QTY','REMAIN_LAYER_SEQ'], # Numerical特徵\n",
    "                                  force_text=None) # Text特徵\n",
    "report_train_with_target = sv.analyze([train_df, 'train'],\n",
    "                                     target_feat='remainDT', # 加入特徵變數\n",
    "                                     feat_cfg=feature_config) \n",
    "report_train_with_target.show_html(filepath='Basic_train_report_with_target.html')                                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compare_report = sv.compare([train_df, 'Training Data'], # 使用compare\n",
    "                            [test_df, 'Test Data'],\n",
    "                            'remainDT',\n",
    "                            feat_cfg=feature_config)\n",
    "\n",
    "compare_report.show_html(filepath='Compare_train_test_report.html')                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_subsets_report = sv.compare_intra(train_df,\n",
    "                                          train_df['LOT_TYPE']=='FDY', # 給條件區分\n",
    "                                          ['FDY', 'ENG'], # 為兩個子資料集命名 \n",
    "                                          target_feat='cytime',\n",
    "                                          feat_cfg=feature_config)\n",
    "\n",
    "compare_subsets_report.show_html(filepath='Compare_male_female_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.dropna(axis='REMAIN_LAYER_SEQ')\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "#train_df1=test_df.dropna(axis='REMAIN_LAYER_SEQ')\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 刪除null值\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "#train_df.dropna(axis='REMAIN_LAYER_SEQ')\n",
    "print(test_df.shape)\n",
    "print(train_df.shape)\n",
    "#train_df1=test_df.dropna(axis='REMAIN_LAYER_SEQ')\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 建立模型\n",
    "print(\"\\n[Info] 建立模型\")  \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "  \n",
    "model = Sequential()  \n",
    "#Dense 意思是這個神經層是全連線層\n",
    "# 輸入層\n",
    "#model.add(Dense(units=3, input_dim=1, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(units=17, input_dim=9, kernel_initializer='random_uniform', activation='relu'))\n",
    "\n",
    "# 隱藏層\n",
    "#model.add(Dense(units=5, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(units=20, kernel_initializer='random_uniform', activation='relu'))\n",
    "model.add(Dense(units=30, kernel_initializer='random_uniform', activation='relu'))\n",
    "model.add(Dense(units=40, kernel_initializer='random_uniform', activation='relu'))\n",
    "\n",
    "model.add(Dense(units=150, kernel_initializer='zeros', activation='relu'))\n",
    "model.add(Dense(units=160, kernel_initializer='zeros', activation='relu'))\n",
    "model.add(Dense(units=170, kernel_initializer='zeros', activation='relu'))\n",
    "\n",
    "# 輸出層\n",
    "model.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "#print(\"\\n[Info] Show model summary...\")  \n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 將特徵欄位進行標準化 \n",
    "    #print(\"\\n[Info] Normalized features...\")  \n",
    "from sklearn import preprocessing  \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# train_df = train_df.drop(['DATA_DATE','ACTUAL_WP_OUT','WS_DATE'], axis=1)  \n",
    "# print(train_df.columns)  \n",
    "# test_df = test_df.drop(['DATA_DATE','ACTUAL_WP_OUT','WS_DATE'], axis=1)  \n",
    "# print(test_df.columns) \n",
    "\n",
    "\n",
    "le=LabelEncoder()\n",
    "for col in train_df[['LOT_ID', 'STATUS', 'CHIPNAME', 'LAYER','PRIORITY', 'LOT_TYPE',  'WS_DATE','IS_MAIN_ROUTE']]:\n",
    "    train_df[col]=le.fit_transform(train_df[col])\n",
    "    test_df[col]=le.fit_transform(test_df[col])\n",
    "print('OK')\n",
    "# minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1))  \n",
    "# scaledFeatures = minmax_scale.fit_transform(train_features)  \n",
    "\n",
    "# rescaling 特徵縮放(0~1) 特徵最小/全距\n",
    "# FS_1= preprocessing.MinMaxScaler().fit(train_features)\n",
    "# result_minmax= FS_1.transform(train_features)\n",
    "\n",
    "#numeric_features = train_df.select_dtypes(include=['numeric'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc=OneHotEncoder()\n",
    "train_df_ohe=enc.fit_transform(train_df).toarray()\n",
    "pd.DataFrame(train_df_ohe)\n",
    "\n",
    "test_df_ohe=enc.fit_transform(test_df).toarray()\n",
    "pd.DataFrame(test_df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 進行訓練\n",
    "\n",
    "# 將 dataframe 轉換為 array\n",
    "ndarray = train_df.values  \n",
    "# print(\"\\n[Info] Translate into ndarray(%s) with shape=%s\" % (ndarray.__class__, str(ndarray.shape)))  \n",
    "# print(\"\\n[Info] Show top 2 records:\\n%s\\n\" % (ndarray[:2]))  \n",
    "# print(\"\\n[Info] ndarray:\")\n",
    "print('ndarray=>',ndarray)\n",
    "\n",
    "# Separate labels with features  \n",
    "train_labels = ndarray[:,0]    # Labels are the values we want to predict\n",
    "train_features = ndarray[:,1:] # Remove the labels from the features\n",
    "print('==========')\n",
    "print(train_labels)\n",
    "print('==========')\n",
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n[Info] 訓練中...\")  \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "#train_history = model.fit(x=train_features, y=train_labels, validation_split=0.1, epochs=10, batch_size=30, verbose=2)  \n",
    "train_history = model.fit(x=train_features, y=train_labels, validation_split=0.1, epochs=200, batch_size=30, verbose=2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}