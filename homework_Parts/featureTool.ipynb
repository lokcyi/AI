{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %% [markdown]\n",
    "# ### MOVE*100/SUM(MOVE) OVER (PARTITION BY MFG_DATE,TOOLG_ID ) as MOVE_P\n",
    "\n",
    "# %%\n",
    "\n",
    "import sys\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "# featuretools for automated feature engineering\n",
    "import featuretools as ft\n",
    "import featuretools.variable_types as vtypes\n",
    "\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import plot_importance \n",
    "\n",
    "train_date = '2020-11-30'\n",
    "final_date = '2021-02-10'\n",
    "intfinal_date = 20210131\n",
    "\n",
    "\n",
    "# %%\n",
    "df_Machine_org = pd.read_csv('./data/Parts_EQP_OutPut_ByMonth.csv')\n",
    "df_parts_org = pd.read_csv('./data/Scm_Parts_TrainingData20210326.csv')\n",
    "\n",
    "\n",
    " \n",
    "df_Machine = df_Machine_org.copy(deep=False)\n",
    "df_parts = df_parts_org.copy(deep=False)\n",
    "\n",
    "\n",
    " \n",
    "df_Product['PPkey'] = df_Product['MFG_DATE'].astype(str)+'_' +df_Product['TOOLG_ID'] +'_' +df_Product['PROD_ID'] \n",
    "df_Product['PRkey'] = df_Product['MFG_DATE'].astype(str)+'_' +df_Product['TOOLG_ID'] \n",
    "df_Tool['TRkey'] = df_Tool['MFG_DATE'].astype(str)+'_' +df_Tool['TOOLG_ID'] \n",
    " \n",
    "df_Product['dtMFG_DATE'] = df_Product['MFG_DATE'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "df_Tool['dtMFG_DATE'] = df_Tool['MFG_DATE'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d'))\n",
    "\n",
    "df_Product = df_Product[df_Product['dtMFG_DATE'] <=  pd.to_datetime(final_date)]\n",
    "df_Tool = df_Tool[df_Tool['dtMFG_DATE'] <=  pd.to_datetime(final_date)] #+ datetime.timedelta(days=1)\n",
    "\n",
    "df_Product = df_Product[df_Product['dtMFG_DATE'] >  pd.to_datetime(train_date)]\n",
    "df_Tool = df_Tool[df_Tool['dtMFG_DATE'] >  pd.to_datetime(train_date)] #+ datetime.timedelta(days=1)\n",
    "\n",
    "\n",
    "# %%\n",
    "# es = ft.EntitySet(id = 'MRATIO')\n",
    "# df_Product\n",
    "\n",
    "\n",
    "# %%\n",
    "# 創建一個空的實體集\n",
    " \n",
    "es = ft.EntitySet(id = 'MRATIO')\n",
    "#Entityset 把多個實體Entity進行合併\n",
    "# Product\n",
    "es = es.entity_from_dataframe(entity_id = 'Product',\n",
    "             dataframe = df_Product,  \n",
    "            #  variable_types={'MFG_DATE': ft.variable_types.Categorical},\n",
    "            make_index = False, \n",
    "            # index=\"ID\",\n",
    "            # index = 'PRkey',\n",
    "            index ='PPkey',\n",
    "            time_index = 'dtMFG_DATE',\n",
    "            variable_types={'MFG_DATE':vtypes.Categorical},\n",
    "\n",
    "             )\n",
    "# Tool\n",
    "es = es.entity_from_dataframe(entity_id = 'Machine',\n",
    "             dataframe = df_Tool,  \n",
    "            #  variable_types={'MFG_DATE': ft.variable_types.Categorical},\n",
    "            # make_index = True, \n",
    "            # index=\"ID\",\n",
    "            index = 'TRkey', \n",
    "            time_index = 'dtMFG_DATE',\n",
    "            variable_types={'MFG_DATE':vtypes.Categorical},\n",
    "            )   \n",
    "\n",
    "\n",
    "# print(es[\"Product\"].variables)\n",
    "# print(es[\"Machine\"].variables)\n",
    "\n",
    "\n",
    "# %%\n",
    "r= ft.Relationship(es['Machine']['TRkey'],es['Product']['PRkey'])\n",
    "es = es.add_relationship(r)\n",
    "\n",
    "# %% [markdown]\n",
    "# \n",
    "# #  target_entity='Machine'\n",
    "\n",
    "# %%\n",
    "# es['Product']['PROD_ID'].interesting_values = df_Product['PROD_ID'].unique()# ['Approved', 'Refused', 'Canceled'] \n",
    "# # df_Product['PROD_ID'].unique()\n",
    "# Tool_matrix,Tool_names= ft.dfs( entityset = es,\n",
    "#                 agg_primitives=['count'], \n",
    "#                 #trans_primitives=['percentile'], \n",
    "#                 # where_primitives = ['cum_count'], \n",
    "#                 #where_primitives = ['mean', 'mode'], \n",
    "\n",
    "#                 # groupby_trans_primitives=['cum_count'], #累積總計、累積總數\n",
    "#                 primitive_options={ \n",
    "#                     # 'percentile': {'include_variables': \n",
    "#                     #                 {'Product': ['PROD_ID'],\n",
    "#                     #                 }},\n",
    "#                     # 'count': {'include_variables': \n",
    "#                     #                 {'Product': ['PROD_ID'],\n",
    "#                     #                 }},\n",
    "#                 },\n",
    "#                 target_entity='Machine')\n",
    "# # Tool_matrix         \n",
    "# # list(Tool_matrix.columns)       \n",
    "\n",
    "\n",
    "# %%\n",
    "# print(es[\"Product\"].variables)\n",
    "# print(es[\"Machine\"].variables)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## target_entity='Product'\n",
    "\n",
    "# %%\n",
    "# https://docs.featuretools.com/en/stable/api_reference.html#aggregation-primitives\n",
    "\n",
    "\n",
    "# %%\n",
    "# es['Product']['PROD_ID'].interesting_values = df_Product['PROD_ID'].unique()# ['Approved', 'Refused', 'Canceled'] \n",
    "# df_Product['PROD_ID'].unique()\n",
    "ProductTool_matrix,ProductTool_names= ft.dfs( entityset = es,\n",
    "                agg_primitives=['std','trend'], #'count',\n",
    "                trans_primitives=[\"is_weekend\"], #weekday\",, \"percentile\"\n",
    "                # where_primitives = ['cum_count'], \n",
    "                #where_primitives = ['mean', 'mode'], \n",
    "\n",
    "                # groupby_trans_primitives=['cum_count'], #累積總計、累積總數\n",
    "                primitive_options={\n",
    "                    'percentile': {\n",
    "                            'include_variables':\n",
    "                                {'Product': ['ProdWIP','ProdHoldWIP']},\n",
    "                            # 'ignore_variables':\n",
    "                            #     {'Porduct': ['MOVE_P']}    \n",
    "                                \n",
    "                      },\n",
    "                        # 'cum_count': {\n",
    "                        #         'include_variables':\n",
    "                        #             {'WIP': ['WIP_QTY']},\n",
    "                        #         'ignore_variables':\n",
    "                        #             {'WIP': ['IS_MAIN_ROUTE']}    \n",
    "                                    \n",
    "                        #             },\n",
    "                        # 'is_weekend': {\n",
    "                        #         'ignore_variables':\n",
    "                        #                 {'WIP': ['ACTUAL_WP_OUT']}}\n",
    "                },\n",
    "\n",
    "                target_entity='Product')\n",
    "# ProductTool_matrix['Machine.COUNT(Product)']         \n",
    "list(ProductTool_matrix.columns)       \n",
    "\n",
    "\n",
    "# %%\n",
    "# ft.primitives.list_primitives()\n",
    "\n",
    "\n",
    "# %%\n",
    "# ProductTool_matrix[['Machine.MFG_DATE']]['Machine.MFG_DATE'].max()\n",
    "\n",
    "\n",
    "# %%\n",
    "#select features that have at least 2 unique values and that are not all null\n",
    "#(feature_matrix, features=None):\n",
    "from featuretools.selection import remove_low_information_features  ## 移除较小信息的特征\n",
    "\n",
    "#Removes columns from a feature matrix that have higher than a set threshold of null values.\n",
    "#(feature_matrix, features=None, pct_null_threshold=0.95):\n",
    "from featuretools.selection import remove_highly_null_features\n",
    "\n",
    "#Removes columns in feature matrix where all the values are the same.\n",
    "#(feature_matrix, features=None, count_nan_as_value=False):\n",
    "from featuretools.selection import remove_single_value_features\n",
    "\n",
    "#Removes columns in feature matrix that are highly correlated with another column.\n",
    "#(feature_matrix, features=None, pct_corr_threshold=0.95,features_to_check=None, features_to_keep=None):\n",
    "from featuretools.selection import remove_highly_correlated_features \n",
    "feature_matrix = ProductTool_matrix.copy(deep=True)\n",
    "features_list = ProductTool_names\n",
    "features1= ProductTool_matrix.copy(deep=True)\n",
    "feature_names1 = ProductTool_names\n",
    "\n",
    "print('features',ProductTool_matrix.shape)\n",
    "# features1, feature_names1 = remove_low_information_features(feature_matrix, features_list)\n",
    "# print('被刪除的欄位 remove_low_information_features',feature_matrix.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_highly_null_features(features1, feature_names1,0.95)\n",
    "# print('被刪除的欄位 remove_highly_null_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_single_value_features(features1, feature_names1)\n",
    "# print('被刪除的欄位 remove_single_value_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_highly_correlated_features(features1, feature_names1,0.95)\n",
    "# print('被刪除的欄位 remove_highly_correlated_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "print(set(feature_matrix) - set(features1))\n",
    "features1['Machine.MFG_DATE']=feature_matrix['Machine.MFG_DATE']\n",
    "\n",
    "\n",
    "# %%\n",
    "features1.head()\n",
    "\n",
    "\n",
    "# %%\n",
    "list(features1.columns)  \n",
    "\n",
    "\n",
    "# %%\n",
    "# # 資料清洗\n",
    "# target_cols=['MOVE_P']\n",
    "# cat_cols = ['PROD_ID','TOOLG_ID']\n",
    "# drop_cols=['PPkey']\n",
    "# aa = [e for e in df_test.columns.to_list() if e not in cat_cols+target_cols+drop_cols]\n",
    "# # .remove(target_cols)\n",
    "# # list(df_test).remove(cat_cols)\n",
    "# df_test[aa]\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "def preHandleDat(df,isTrain=True):\n",
    "    from sklearn.preprocessing import StandardScaler #平均&變異數標準化 平均值為0，方差為1。\n",
    "    from sklearn.preprocessing import MinMaxScaler #最小最大值標準化[0,1]\n",
    "    from sklearn.preprocessing import RobustScaler #中位數和四分位數標準化\n",
    "    from sklearn.preprocessing import MaxAbsScaler #絕對值最大標準化\n",
    "\n",
    "    #=================\n",
    "    #刪除不必要的欄位\n",
    "    #=================\n",
    "    # drop_cols=['MFG_DATE','PPkey','PRkey','TRkey','dtMFG_DATE','dtMFG_DATE']\n",
    "    drop_cols=[ 'Machine.MFG_DATE','Machine.TOOLG_ID','PPkey','PRkey']\n",
    " \n",
    " \n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    #drop_cols=['MFG_DATE','ARRIVAL_WIP_QTY','MOVE_QTY','PROCESS_TIME','C_TC','UP_TIME','C_UP_TIME','LOT_SIZE','C_LOT_SIZE']\n",
    "    # df = df.drop(drop_cols, axis=1)\n",
    "    \n",
    "    target_cols=['MOVE_P']\n",
    "    cat_cols = ['PROD_ID','TOOLG_ID']\n",
    "    num_cols =[e for e in df.columns.to_list() if e not in cat_cols+target_cols]\n",
    "    # print(num_cols)\n",
    "    # print(df[num_cols])\n",
    "    # num_cols=[ 'ProdWIP', 'ProdHoldWIP',\n",
    "    #    'Machine.IS_HOLIDAY', 'Machine.M_NUM',\n",
    "    #    'Machine.UP_TIME', 'Machine.C_UP_TIME', 'Machine.LOT_SIZE',\n",
    "    #    'Machine.C_LOT_SIZE', 'Machine.EQP_UTIL', 'Machine.C_EQP_UTIL',\n",
    "    #    'Machine.EQP_AVAIL_RATE', 'Machine.TC', 'Machine.PROCESS_TIME',\n",
    "    #     'Machine.INLINE_CT_BY_LOT',\n",
    "    #      'Machine.WIP_QTY',  \n",
    "    #    'Machine.ARRIVAL_WIP_QTY',  'Machine.RUN_WIP_RATIO',\n",
    "    #    'Machine.CLOSE_WIP_QTY',   'Machine.C_TC',\n",
    "    #    'Machine.C_CLOSE_WIP',  \n",
    "    #       'Machine.HOLD_RATE',\n",
    "    #    'Machine.ENG_LOT_RATE', 'Machine.HOT_LOT_RATE',\n",
    "    #    'Machine.BACKUP_BY_RATE', 'Machine.BACKUP_FOR_RATE',\n",
    "    #    'Machine.REWORK_LOT_RATE', 'Machine.QUE_LOT_RATE',\n",
    "    #       'Machine.CHANGE_RECIPE',\n",
    "    #       'Machine.COUNT(Product)'] \n",
    "# 'Machine.RUN_WIP', 'Machine.SAMPLING_RATE', 'Machine.C_TOOLG_LOADING', 'Machine.MANAGEMENT_WIP_QTY', 'Machine.C_TOOL_LOADING', 'Machine.NO_HOLD_QTY', 'Machine.DISPATCHING', 'Machine.NUM_RECIPE', 'Machine.INLINE_CT_BY_WAFER', 'Machine.PROCESS_JOBTIME', 'Machine.BATCH_SIZE'\n",
    "    \n",
    "    #========================\n",
    "    # 缺漏值填空\n",
    "    #========================\n",
    "    #df_train['PROCESS_TIME'] = df_train['PROCESS_TIME'].replace(to_replace=0, method='ffill')\n",
    "    df = df.fillna(df.median())\n",
    "    #df = df.fillna(method='bfill') #往後\n",
    "    #df = df.fillna(method='ffill') #往前\n",
    "    # df = df.fillna(df.mean())  #用平均值取代 nan   \n",
    "     \n",
    "    #df['ColA'].fillna(value=0, inplace=True) #用 0 取代 nan\n",
    "    #df['ColA'].fillna(value=df.groupby('ColB')['ColA'].transform('mean'), inplace=True) #利用 groupby()同一group 的平均值\n",
    "    df = df.fillna(value=0)\n",
    "    #==================================================\n",
    "    #1.特徵縮放\n",
    "    #==================================================\n",
    "\n",
    "    df_train_scal = df.copy(deep=False)\n",
    "    global df_cols\n",
    "    df_cols = df_train_scal.columns\n",
    "    \n",
    "    if isTrain:\n",
    "        \n",
    "        \n",
    "        #rescaling 特徵縮放 StandardScaler-------------------------------------\n",
    "        # scaler = StandardScaler()\n",
    "        # scaler.fit(df_train[num_cols])\n",
    "        # df_train_scal[num_cols]= scaler.transform(df_train_scal[num_cols])\n",
    "    \n",
    "        #rescaling 特徵縮放 MinMaxScaler-------------------------------------\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df[num_cols])\n",
    "        df_train_scal[num_cols]= scaler.transform(df_train_scal[num_cols])\n",
    "\n",
    "        #rescaling 特徵縮放 RobustScaler-------------------------------------\n",
    "        # scaler = RobustScaler()\n",
    "        # scaler.fit(df_train[num_cols])\n",
    "        # df_train_scal[num_cols] = scaler.transform(df_train_scal[num_cols])\n",
    "        \n",
    "        dump(scaler, open('scaler.pkl', 'wb'))\n",
    "        # # save the scaler\n",
    "        # dump(scaler, open('scaler.pkl', 'wb'))\n",
    "    else:\n",
    "        # load the scaler\n",
    "        scaler = load(open('scaler.pkl', 'rb'))\n",
    "        df_train_scal[num_cols] = scaler.transform(df[num_cols])\n",
    "     #==================================================\n",
    "    #2.one hot encoder\n",
    "    #==================================================\n",
    "    # target_cols=['MOVE_QTY']\n",
    " \n",
    "    global df2_train_eh_before\n",
    "    df_train_eh =pd.get_dummies(df_train_scal.drop(target_cols, axis=1),columns=cat_cols)\n",
    "    # save the scaler\n",
    "\n",
    "\n",
    "    if isTrain:\n",
    "        df2_train_eh_before = df_train_eh.copy(deep=False)\n",
    "    else:\n",
    "        df_train_eh = df_train_eh.reindex(columns = df2_train_eh_before.columns, fill_value=0)\n",
    "\n",
    "    df_train_scal.to_csv('./df_train_scal.csv')\n",
    "           \n",
    "    X_dropped = np.asarray(df_train_eh)\n",
    "    Y_dropped = np.asarray(df[target_cols]) \n",
    "    # print(df_train_eh)\n",
    "    return X_dropped,Y_dropped\n",
    "\n",
    "\n",
    "# %%\n",
    "# preHandleDat(feature_matrix,True)\n",
    "\n",
    "\n",
    "# %%\n",
    "def iqrfilter(df, colname, bounds = [.25, .75]):\n",
    "    s = df[colname]\n",
    "    Q1 = df[colname].quantile(bounds[0])\n",
    "    Q3 = df[colname].quantile(bounds[1])\n",
    "    IQR = Q3 - Q1\n",
    "    # print(IQR,Q1,Q3,Q1 - 1.5*IQR,Q3+ 1.5 * IQR)\n",
    "    if bounds[0]==0:\n",
    "        return df[~s.clip(*[Q1,Q3+ 1.5 * IQR]).isin([Q1,Q3+ 1.5 * IQR])]\n",
    "    else:\n",
    "        return df[~s.clip(*[Q1 - 1.5*IQR,Q3+ 1.5 * IQR]).isin([Q1 - 1.5*IQR,Q3+ 1.5 * IQR])]\n",
    "\n",
    "\n",
    "# %%\n",
    "def trainLR(df):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import joblib\n",
    "\n",
    "    X,Y = preHandleDat(df,True)\n",
    "    \n",
    "    #拆分train validation set\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,Y,test_size =0.01,random_state=123)\n",
    "    \n",
    "    model = LinearRegression(fit_intercept=False, normalize=False, copy_X=False) #fit_intercept=False\n",
    "   \n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    \n",
    "    #print(y_predict)\n",
    "    print(\"r2:\", model.score(X_test, y_test))# 拟合优度R2的输出方法\n",
    "    print(\"MAE:\", metrics.mean_absolute_error(y_test, y_predict))# 用Scikit_learn计算MAE\n",
    "    print(\"MSE:\", metrics.mean_squared_error(y_test, y_predict)) # 用Scikit_learn计算MSE\n",
    "    print(\"RMSE:\", np.sqrt(metrics.mean_squared_error(y_test, y_predict)))# 用Scikit_learn计算RMSE\n",
    "    print(\"intercept_ :\",model.intercept_)\n",
    "    # print(\"coef_\", model.coef_)\n",
    "    joblib.dump(model, 'LR_model')\n",
    "    # print(1- abs((def_result['predict'] - def_result['TRCT'])/def_result['TRCT'] ))#計算準確率分數)\n",
    "\n",
    "    # print((y_predict-y_test)/y_test)\n",
    "    # reg = LinearRegression().fit(X, y)\n",
    " \n",
    "    # line_poly = poly.transform(line)\n",
    "    # plt.plot(line, reg.predict(line), label='linear regression')\n",
    "    # plt.plot(X[:, 0], y, 'o', c='k')\n",
    "    # plt.ylabel(\"Regression output\")\n",
    "    # plt.xlabel(\"Input feature\")\n",
    "    # plt.legend(loc=\"best\")\n",
    " \n",
    "\n",
    "    return y_predict\n",
    "\n",
    "\n",
    "# %%\n",
    "def testLR(df):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    X_test,y_test = preHandleDat(df,False)\n",
    " \n",
    "    loaded_model = joblib.load('LR_model')\n",
    "    y_predict = loaded_model.predict(X_test)\n",
    "    df_result=df.copy(deep=True)\n",
    "    df_result['predict'] = y_predict\n",
    "    \n",
    "    loaded_model.score(X_test, y_test)\n",
    "    print(\"r2:\", loaded_model.score(X_test, y_test))\n",
    "    print('acc:',accsum(df_result))\n",
    "\n",
    "    t = np.arange(len(X_test))# 创建t变量\n",
    "    drawModelResult('LR','',t,y_test, y_predict,'./Result/LR.png')\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# %%\n",
    "def trainXG(df):\n",
    "    import xgboost as xgb\n",
    "    \n",
    "    import joblib\n",
    "\n",
    "    X,Y = preHandleDat(df,True)\n",
    "    \n",
    "    #拆分train validation set\n",
    "    X_train, X_test,y_train,y_test = train_test_split(X,Y,test_size =0.2,random_state=587)\n",
    "    cv_params = {'n_estimators': [300,400,500,600],'max_depth':[7,11,13,15,17],'min_child_weight':[1,3,5,7,9]}\n",
    "    other_params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,\n",
    "    'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "    model = xgb.XGBRegressor(**other_params)\n",
    "    bst_model=model\n",
    "    # optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='r2', cv=5, verbose=1, n_jobs=4)\n",
    "\n",
    "    # optimized_GBM.fit(X_train, y_train) \n",
    "\n",
    "    # test_score = optimized_GBM.score(X_test,y_test)\n",
    "\n",
    "    # print('test 得分:{0}'.format(test_score))\n",
    "    # # evalute_result = optimized_GBM.grid_scores_\n",
    "    # # print('每輪迭代執行結果:{0}'.format(evalute_result))\n",
    "    \n",
    "    # print('引數的最佳取值：{0}'.format(optimized_GBM.best_params_))\n",
    "    # print('最佳模型得分:{0}'.format(optimized_GBM.best_score_))\n",
    "    # print('cv_results_',optimized_GBM.cv_results_)\n",
    "\n",
    "    # bst_model = optimized_GBM.estimator\n",
    "    # print(bst_model)\n",
    "\n",
    "    \n",
    "    bst_model.fit(X_train, y_train)\n",
    "    # plot_importance(bst_model)#,max_num_features=10)\n",
    "    # plt.show()\n",
    "\n",
    "    joblib.dump(bst_model, 'XG_model')\n",
    "\n",
    "\n",
    "# %%\n",
    "def testXG(df):\n",
    "    X_test,y_test = preHandleDat(df,False)\n",
    "    # print(X_test)\n",
    "    #print(X_test,y_test)\n",
    "    df_result=df.copy(deep=True)\n",
    "    loaded_model = joblib.load('XG_model')\n",
    "    y_predict = loaded_model.predict(X_test)\n",
    "    df_result['predict'] = y_predict\n",
    "    \n",
    "    loaded_model.score(X_test, y_test)\n",
    "    print(\"r2:\", loaded_model.score(X_test, y_test))\n",
    "    print('acc:',accsum(df_result))\n",
    "    \n",
    " \n",
    "    t = np.arange(len(X_test))# 创建t变量\n",
    "    drawModelResult('LR','',t,y_test, y_predict,'./Result/XGtest.png')\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "# %%\n",
    "def trainNN(df):\n",
    "    import tensorflow as tf\n",
    "    save_model_tool = './NN/training_model2.h5'\n",
    "    # save_model_tool = getSavePath(df['TOOLG_ID'].iloc[0],save_model)\n",
    "    df_result = df.copy(deep=False)\n",
    "    X_dropped, Y_dropped = preHandleDat(df_result,True)\n",
    "    # tuneNN(X_dropped,Y_dropped)\n",
    "    \n",
    "    # 拆分train validation set NN fit 可以自己拆\n",
    "    # X_train, X_test,y_train,y_test = train_test_split(X_dropped,Y_dropped,test_size =0.1,random_state=587)\n",
    "    \n",
    "    #1.建立模型(Model)\n",
    "    #將Layer放入Model中\n",
    "    # Activation Functions\n",
    "    # A.softmax：值介於 [0,1] 之間，且機率總和等於 1，適合多分類使用。\n",
    "    # B.sigmoid：值介於 [0,1] 之間，且分布兩極化，大部分不是 0，就是 1，適合二分法。\n",
    "    # C.Relu (Rectified Linear Units)：忽略負值，介於 [0,∞] 之間。\n",
    "    # D.tanh：與sigmoid類似，但值介於[-1,1]之間，即傳導有負值。\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(units=16,input_shape=[X_dropped.shape[1]]), # units：输出维度\n",
    "        # tf.keras.layers.Dense(units=32),\n",
    "        # tf.keras.layers.Dense(units=64),\n",
    "        # tf.keras.layers.Dropout(0.2),\n",
    "        # tf.keras.layers.Dense(units=32),\n",
    "        # tf.keras.layers.Dropout(0.2),\n",
    "        # tf.keras.layers.Dense(units=1)\n",
    "\n",
    "        \n",
    "        tf.keras.layers.Dense(units=16, kernel_initializer='random_uniform', activation='relu'),\n",
    "        # tf.keras.layers.Dense(units=16, kernel_initializer='random_uniform', activation='relu'),\n",
    "\n",
    "        #tf.keras.layers.Dense(units=1)\n",
    "        tf.keras.layers.Dense(units=1, kernel_initializer='random_uniform',activation='relu')\n",
    "        \n",
    "\n",
    "        ])\n",
    "    #model.summary()\n",
    "    #======================================================================================\n",
    "    # 定義 tensorboard callback\n",
    "    tensorboard_callback = [tf.keras.callbacks.TensorBoard(log_dir='D:/Projects/AI/POC/homework/logs2')]\n",
    "    #======================================================================================\n",
    "    #2. 確立目標及求解方法：以compile函數定義損失函數(loss)、優化函數(optimizer)及成效衡量指標(mertrics)。\n",
    "\n",
    "        #compile()方法來指定損失函數與優化函數，也可以額外指定一系列的訓練和評估期間計算評量標準\n",
    "        #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  \n",
    "        \n",
    "        # # model.compile( loss = root_mean_squared_error,optimizer = \"rmsprop\",metrics =[\"accuracy\"],callbacks=tensorboard_callback) \n",
    "        # model.compile(loss = root_mean_squared_error,metrics =[\"accuracy\"],callbacks=tensorboard_callback) \n",
    "        # \n",
    "    #--------------------------\n",
    "    #[Keras 損失函數(loss)]\n",
    "        #   A.均方誤差(mean_squared_error)----mean_squared_error-------------------------------\n",
    "        #   B.Hinge Error (hinge)：是一種單邊誤差，不考慮負值，適用於『支援向量機』(SVM)的最大間隔分類法(maximum-margin classification)\n",
    "        #  C.Cross Entropy (categorical_crossentropy)：當預測值與實際值愈相近，損失函數就愈小，反之差距很大，就會更影響損失函數的值，這篇文章 主張要用 Cross Entropy 取代 MSE，因為，在梯度下時，Cross Entropy 計算速度較快，其他變形包括 sparse_categorical_crossentropy、binary_crossentropy。\n",
    "        #  D.其他還有 logcosh、kullback_leibler_divergence、poisson、cosine_proximity 等。\n",
    "    #--------------------------\n",
    "    #[優化函數(Optimizer)]\n",
    "    #   A. 隨機梯度下降法(Stochastic Gradient Descent, SGD)：就是利用偏微分，逐步按著下降的方向，尋找最佳解。它含以下參數：\n",
    "    #        Learning Rate (lr)：逼近最佳解的學習速率，速率訂的太小，計算最佳解的時間花費較長，訂的太大，可能會在最佳解兩                               旁擺盪，找不到最佳解。\n",
    "    #        momentum：更新的動能，一開始學習速率可以大一點，接近最佳解時，學習速率步幅就要小一點，一般訂為0.5，不要那麼大時，可改為 0.9。\n",
    "    #        decay：每次更新後，學習速率隨之衰減的比率。\n",
    "    #        nesterov：是否使用 Nesterov momentum，請參考 http://blog.csdn.net/luo123n/article/details/48239963 。\n",
    "    #  B.Adam：一般而言，比SGD模型訓練成本較低，請參考『Adam - A Method for Stochastic Optimization』，包含相關參數建議值，含以下參數：\n",
    "\n",
    "    #        lr：逼近最佳解的學習速率，預設值為0.001。\n",
    "    #        beta_1：一階矩估計的指數衰減因子，預設值為0.9。\n",
    "    #        beta_2：二階矩估計的指數衰減因子，預設值為0.999。\n",
    "    #        epsilon：為一大於但接近 0 的數，放在分母，避免產生除以 0 的錯誤，預設值為1e-08。\n",
    "    #        decay：每次更新後，學習速率隨之衰減的比率。\n",
    "    # model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01)\n",
    "    #                     , metrics= ['accuracy'],callbacks=tensorboard_callback) \n",
    "    # model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD()\n",
    "    #                     , metrics= ['accuracy'],callbacks=tensorboard_callback) \n",
    "    #========================================================================\n",
    "    # SGD\n",
    "    sgd = tf.keras.optimizers.SGD(lr=0.20, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    # 随机梯度下降优化器。\n",
    "    # 包含扩展功能的支持： - 动量（momentum）优化, - 学习率衰减（每次参数更新后） - Nestrov 动量 (NAG) 优化\n",
    "    # 参数\n",
    "    # lr: float >= 0. 学习率。\n",
    "    # momentum: float >= 0. 参数，用于加速 SGD 在相关方向上前进，并抑制震荡。\n",
    "    # decay: float >= 0. 每次参数更新后学习率衰减值。\n",
    "    # nesterov: boolean. 是否使用 Nesterov 动量。\n",
    "    #========================================================================\n",
    "    # compile 編譯模型\n",
    "    # model.compile(loss='mean_squared_error', optimizer='adam'\n",
    "    #                     , metrics= ['accuracy','mse', 'mae', 'mape'],callbacks=tensorboard_callback) 」‘「＝、\n",
    "    \n",
    "    # model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.SGD(lr=0.01,decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    #                     , metrics= ['accuracy','mse', 'mae', 'mape'],callbacks=tensorboard_callback) \n",
    "    # model.compile(loss=root_mean_squared_error, optimizer=tf.keras.optimizers.Adam(lr=0.01)\n",
    "    #                     , metrics = [root_mean_squared_error, 'mae', 'mape'])\n",
    "    \n",
    "    # model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(lr=0.01)\n",
    "    #                     , metrics = [root_mean_squared_error, 'mae', 'mape'])   \n",
    "    model.compile(loss='mean_absolute_error', optimizer=tf.keras.optimizers.Adam(lr=0.01) #, optimizer=tf.keras.optimizers.SGD(lr=0.2)\n",
    "                        , metrics = [ 'mae', 'mape'])   \n",
    "    \n",
    "    #model = tuningNN(model,X_dropped,Y_dropped)                                            \n",
    "    #======================================================================================\n",
    "\n",
    "    #3.訓練 fit：以compile函數進行訓練，指定訓練的樣本資料(x, y)，並撥一部分資料作驗證，還有要訓練幾個週期、訓練資料的抽樣方式。\n",
    "    # train_history = model.fit(x=X_dropped, y=Y_dropped,\n",
    "    #                validation_data=(X_dropped_test, Y_dropped_test), # Use this instead\n",
    "    #                 epochs=50, batch_size=30, verbose=2) #validation_split=0.1, 用最後的10%資料驗證 batch_size=200: 每一批次200筆資料 verbose=2: 顯示訓練過程\n",
    "    train_history = model.fit(x=X_dropped, y=Y_dropped,\n",
    "                #validation_data=(X_dropped_test, Y_dropped_test), # Use this instead\n",
    "                validation_split=0.10, epochs=75, batch_size=50, verbose=0) #,shuffle=True validation_split=0.1, 用最後的10%資料驗證 batch_size=200: 每一批次200筆資料\n",
    "    # early_stopping = tf.keras.callbacks.EarlyStopping(patience=25)\n",
    "    # train_history = model.fit(x=X_dropped, y=Y_dropped, validation_split=0.1, epochs=200, batch_size=30, verbose=2)\n",
    "    # train_history = model.fit(X_dropped, Y_dropped, epochs=20, verbose=True)\n",
    "    \n",
    "    model.save(save_model_tool)\n",
    "    # model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(0.01))\n",
    "\n",
    "    #評估(Evaluation)：訓練完後，計算成效。\n",
    "    # 顯示訓練成果(分數)\n",
    "    # scores = model.evaluate(X_dropped, Y_dropped) \n",
    "    \n",
    "    # print(\"Finished training the model\",scores)\n",
    "    print(train_history.history.keys())\n",
    "    #dict_keys(['loss', 'accuracy', 'mse', 'mae', 'mape', 'val_loss', 'val_accuracy', 'val_mse', 'val_mae', 'val_mape'])\n",
    "    \n",
    "    # 當RMSE收斂至接近0.02，且MAPE接近10%，即完成模型之訓練\n",
    "    figure, axis_1 = plt.subplots()\n",
    "    plt.title(df['TOOLG_ID'].iloc[0]) # title\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.ylabel(\"Loss Magnitude\")\n",
    "\n",
    "    loss = axis_1.plot(train_history.history['loss'], label = 'loss')\n",
    "\n",
    "    #plt.ylabel(\"accuracy\")\n",
    "    # ac =axis_1.plot(train_history.history['accuracy'], label = 'accuracy' )# 準確度\n",
    "    # ac =axis_1.plot(train_history.history['mse'], label = 'mse' )# 準確度\n",
    "    \n",
    "    \n",
    "    \n",
    "    # mse = axis_1.plot(train_history.history['mse'], label = 'mse' ) \n",
    "    # vallossaxis_1= axis_1.plot(train_history.history['val_loss'], label = 'val_loss' )# 準確度\n",
    "    axis_2 = axis_1.twinx()\n",
    "    # mse = axis_2.plot(train_history.history['mse'], label = 'mse',color='red' ) \n",
    "    mse = axis_2.plot(train_history.history['mape'], label = 'mape',color='red' ) \n",
    "    \n",
    "    # mape = axis_2.plot(train_history.history['mape'], label = 'mape' )# 準確度 接近10%\n",
    "        \n",
    "    axis_1.legend(loc='upper left',fontsize='large')\n",
    "    axis_2.legend(loc='upper right',fontsize='large')\n",
    "    # plt.legend((ac, valloss, mse, mape),\n",
    "    #        ('Low accuracy', 'val_loss', 'mse', 'mape'),\n",
    "    #        scatterpoints=1,\n",
    "    #        loc='upper right',\n",
    "    #     #    ncol=3,\n",
    "    #        fontsize=14)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error #均方誤差\n",
    "    from sklearn.metrics import mean_absolute_error #平方絕對誤差\n",
    "    from sklearn.metrics import r2_score#R square\n",
    "    #呼叫\n",
    "    y_predict = model.predict(X_dropped)\n",
    "    #print(np.count_nonzero(~np.isnan(Y_dropped)),np.count_nonzero(~np.isnan(y_predict)))\n",
    "    Y_dropped = np.nan_to_num(np.nan, copy=True)\n",
    "    # Y_dropped = np.nan_to_num(np.nan, copy=True)\n",
    "    \n",
    "    # print(\"mean_squared_error\",mean_squared_error(Y_dropped,y_predict))\n",
    "    # print(\"mean_absolute_error\",mean_absolute_error(Y_dropped,y_predict))\n",
    "    # print(\"r2_score\",r2_score(Y_dropped,y_predict))\n",
    "    #return df_test_eh\n",
    "\n",
    "\n",
    "# %%\n",
    "def testNN(df):\n",
    "    import tensorflow as tf\n",
    "    save_model_tool = './NN/training_model2.h5'\n",
    "    model = tf.keras.models.load_model(save_model_tool)\n",
    "    df_result=df.copy(deep=True)\n",
    "    \n",
    "    X_dropped,Y_dropped = preHandleDat(df,False)\n",
    "    y_predict = model.predict(X_dropped)\n",
    "    df_result['predict'] = y_predict # 預測\n",
    "    \n",
    "    print('acc:',accsum(df_result))\n",
    "    \n",
    " \n",
    "    t = np.arange(len(X_dropped))# 创建t变量\n",
    "    drawModelResult('LR','',t,Y_dropped, y_predict,'./Result/NNtest.png')\n",
    "    \n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# %%\n",
    "def drawModelResult(modelType,TOOLG_ID,x,y_actual,y_predict28 ,imagepath):\n",
    "\n",
    "    plt.figure(figsize=(20, 5))# 圖片尺寸\n",
    "        \n",
    "    plt.title('TOOLG_ID:'+ TOOLG_ID+ \" (\"+modelType+\")\")\n",
    "    \n",
    "    plt.plot(x, y_actual, 'b', linewidth=0, marker='x', label='actual') \n",
    "    plt.plot(x, y_predict28, 'r', linewidth=1, marker='.', label='predict on 28 days')  \n",
    "    # plt.plot(x, y_predict_actrual, 'k', linewidth=1, marker='.', label='predict on actual') \n",
    "    plt.legend()\n",
    "    # if TOOLG_ID=='PK_DUVKrF' :\n",
    "    #     plt.yticks(np.linspace(0.15,0.45,9))\n",
    "    # elif TOOLG_ID=='XE_Sorter' :\n",
    "    #     plt.yticks(np.linspace(0.0,0.12,9))\n",
    "    # elif TOOLG_ID=='MA_Alps' :\n",
    "    #     plt.yticks(np.linspace(0.0,0.5,9))\n",
    "    # elif TOOLG_ID=='PW_PIX' :\n",
    "    #     plt.yticks(np.linspace(0.0,0.1,9))    \n",
    "    plt.savefig(imagepath)\n",
    "\n",
    "\n",
    "# %%\n",
    "def accsum(def_result,target_col='MOVE_P'):\n",
    "    _accsum=0\n",
    "    def_result[def_result[target_col] ==0.0][target_col]  =0.0001\n",
    "    def_result[def_result['predict'] <0]['predict']  =0\n",
    "    for index,row in def_result.iterrows():\n",
    "        if row[target_col] <0 :\n",
    "            row[target_col]  =0.00001\n",
    "        if row[target_col] ==0.0:\n",
    "            row[target_col]  =0.00001\n",
    "        # print(row['TRCT'] )\n",
    "        if 1- abs((row['predict'] - row[target_col])/row[target_col] ) >0 :\n",
    "            \n",
    "            _accsum+=(1- abs((row['predict'] - row[target_col])/row[target_col] ))\n",
    "    \n",
    "    return _accsum/def_result.shape[0]\n",
    "\n",
    "\n",
    "# %%\n",
    "# es.plot()\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "df = pd.DataFrame(features1.to_records())# <==很重要~~~~~\n",
    "\n",
    "# df['Machine.MFG_DATE'] = df['Machine.MFG_DATE'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d')) \n",
    "\n",
    "\n",
    "# pd.to_datetime(final_date)\n",
    "#df['Machine.MFG_DATE'].max()\n",
    "\n",
    "\n",
    "# %%\n",
    "# df.columns\n",
    "\n",
    "\n",
    "# %%\n",
    "df_train = df[ df['Machine.MFG_DATE'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d')) != pd.to_datetime(final_date)]\n",
    "df_test = df[ df['Machine.MFG_DATE'].apply(lambda x: pd.to_datetime(str(x), format='%Y%m%d')) == pd.to_datetime(final_date)]\n",
    "\n",
    "\n",
    "# %%\n",
    "df_train =iqrfilter(df_train,'Machine.M_NUM',[0.25, 1]) \n",
    "\n",
    "\n",
    "# %%\n",
    "# df_train.columns\n",
    "\n",
    "\n",
    "# %%\n",
    "trainLR(df_train)\n",
    "testLR(df_train)\n",
    "testLR(df_test)\n",
    "\n",
    "\n",
    "# %%\n",
    "trainXG(df_train)\n",
    "testXG(df_train)\n",
    "testXG(df_test)\n",
    "\n",
    "\n",
    "# %%\n",
    "trainNN(df_train)\n",
    "testNN(df_train)\n",
    "testNN(df_test)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\n",
    "# default_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\",\"num_words\",\"haversine\",\"num_characters\"]\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# target 為Product 沒有意義\n",
    "# default_agg_primitives=[\"std\"]\n",
    "# default_trans_primietives = []\n",
    "\n",
    "# feature_matrix,feature_names= ft.dfs( entityset = es,\n",
    "#                 target_entity='Product',\n",
    "#                 trans_primitives = default_trans_primietives,\n",
    "#                 agg_primitives=default_agg_primitives,\n",
    "#                 primitive_options={ \n",
    "#                     #'std': {'ignore_entities': ['cohorts', 'log']}, \n",
    "#                     'std': {'include_variables': \n",
    "#                                 {'Product': ['ProdMove', 'MOVE_P'],\n",
    "#                                 'Machine': ['M_NUM'],\n",
    "#                                 }},\n",
    "\n",
    "#                 },\n",
    "\n",
    "#             max_depth=2,\n",
    "#             features_only=False,verbose=True) \n",
    "\n",
    "\n",
    "# %%\n",
    "# Default primitives from featuretools\n",
    "#default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\", \"count\", \"percent_true\", \"num_unique\", \"mode\"]\n",
    "# default_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\", \"haversine\",   \"characters\"]\n",
    "#default_trans_primitives =  [\"day\", \"year\", \"month\", \"weekday\"]\n",
    "\n",
    "default_trans_primitives = [\"weekday\"]\n",
    "default_agg_primitives=[\"std\"]\n",
    "feature_matrix2,feature_names2= ft.dfs( entityset = es,\n",
    "                target_entity='Machine',\n",
    "                trans_primitives = default_trans_primitives, #對母表 單一筆資料去處理\n",
    "                agg_primitives=default_agg_primitives, #對 子表做分析\n",
    "                primitive_options={ \n",
    "                    #'std': {'ignore_entities': ['cohorts', 'log']}, \n",
    "                    'std': {'include_variables': \n",
    "                                {'Product': ['ProdMove', 'MOVE_P'],#子表(1:N的表去計算)\n",
    "                                'Machine': ['M_NUM'], ## 沒有用因為 要針對子表(1:N的表去計算) \n",
    "                                }},\n",
    "                },\n",
    "            max_depth=2,\n",
    "            features_only=False,verbose=True) \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "#驗證資料\n",
    "print(feature_matrix2[['MFG_DATE','WEEKDAY(dtMFG_DATE)','TOOLG_ID','STD(Product.MOVE_P)','STD(Product.ProdMove)']].head(1))\n",
    "print(df_Product[df_Product['PRkey']=='20200122_MA_Alps'].std())\n",
    " \n",
    "# df_Product[(df_Product.PROD_ID=='4GDDR4_25A') & (df_Product.MFG_DATE==20200122)].std()\n",
    "\n",
    "\n",
    "# %%\n",
    "#feature_matrix2\n",
    "\n",
    "\n",
    "# %%\n",
    "# # Feature Selection\n",
    "\n",
    "# # Threshold for removing correlated variables\n",
    "# threshold = 0.95\n",
    "\n",
    "# # Absolute value correlation matrix\n",
    "# corr_matrix = feature_names2.corr().abs()\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# upper.head(50)\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "# #select features that have at least 2 unique values and that are not all null\n",
    "# #(feature_matrix, features=None):\n",
    "# from featuretools.selection import remove_low_information_features  ## 移除较小信息的特征\n",
    "\n",
    "# #Removes columns from a feature matrix that have higher than a set threshold of null values.\n",
    "# #(feature_matrix, features=None, pct_null_threshold=0.95):\n",
    "# from featuretools.selection import remove_highly_null_features\n",
    "\n",
    "# #Removes columns in feature matrix where all the values are the same.\n",
    "# #(feature_matrix, features=None, count_nan_as_value=False):\n",
    "# from featuretools.selection import remove_single_value_features\n",
    "\n",
    "# #Removes columns in feature matrix that are highly correlated with another column.\n",
    "# #(feature_matrix, features=None, pct_corr_threshold=0.95,features_to_check=None, features_to_keep=None):\n",
    "# from featuretools.selection import remove_highly_correlated_features \n",
    "# feature_matrix = feature_matrix2\n",
    "# features_list =feature_names2\n",
    "\n",
    "\n",
    "# print('features',feature_matrix.shape)\n",
    "# features1, feature_names1 = remove_low_information_features(feature_matrix, features_list)\n",
    "# print('被刪除的欄位 remove_low_information_features',feature_matrix.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_highly_null_features(features1, feature_names1,0.95)\n",
    "# print('被刪除的欄位 remove_highly_null_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_single_value_features(features1, feature_names1)\n",
    "# print('被刪除的欄位 remove_single_value_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "# features1, feature_names1 = remove_highly_correlated_features(features1, feature_names1,0.95)\n",
    "# print('被刪除的欄位 remove_highly_correlated_features',features1.shape)\n",
    "# print('Removed %d features from training features' % (feature_matrix.shape[1] - features1.shape[1])) \n",
    "# print(set(feature_matrix) - set(features1))\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # 客制功能\n",
    "\n",
    "# %%\n",
    "from featuretools.primitives import make_agg_primitive, make_trans_primitive\n",
    "from featuretools.variable_types import Text, Numeric,Boolean,Variable\n",
    "\n",
    "\n",
    "# %%\n",
    "# custom function so the name of the feature prints out correctly\n",
    "def make_name(self):\n",
    "    return \"%s_goal_last_%d\" % (self.kwargs['Qty1'], self.kwargs['Qty2'])\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "def compare_Qty(Qty1,Qty2):\n",
    "   return Qty1>Qty2\n",
    "CompareMove = make_trans_primitive(function=compare_Qty,\n",
    "                                          input_types=[Numeric, Numeric],\n",
    "                                          return_type=Boolean,\n",
    "                                          description=\"compare_Qty\"\n",
    "                                          #cls_attributes={\"generate_name\": make_name, \"uses_full_entity\":True}\n",
    "                                          )\n",
    "\n",
    "input_vars = [es[\"Machine\"][\"MOVE_QTY\"], es[\"Machine\"][\"WIP_QTY\"]]\n",
    "\n",
    "\n",
    "# Compare_Move = CompareMove(*input_vars)\n",
    "# #Compare_Move = CompareMove(Qty1=es[\"Machine\"][\"MOVE_QTY\"], Qty2=es[\"Machine\"][\"WIP_QTY\"])\n",
    "\n",
    "# features = [Compare_Move]\n",
    "\n",
    "\n",
    "# fm = ft.calculate_feature_matrix(entityset=es, features=features) \n",
    "\n",
    "\n",
    "feature_matrix, features = ft.dfs(entityset=es,target_entity=\"Product\",agg_primitives=[],\n",
    "                        trans_primitives=[CompareMove],\n",
    "                         primitive_options={ \n",
    "                            #'std': {'ignore_entities': ['cohorts', 'log']}, \n",
    "                        'CompareMove': {'include_variables': \n",
    "                                        {'Product': ['ProdMove', 'MOVE_P'],\n",
    "                                        }},\n",
    "\n",
    "                        },\n",
    "                        max_depth=2)\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_matrix\n",
    "\n",
    "\n",
    "# %%\n",
    "# Feature lineage graphs\n",
    "# Understanding Feature Output¶\n",
    "# In general, Featuretools references generated features through the feature name. In order to make features easier to understand, Featuretools offers two additional tools, featuretools.graph_feature() and featuretools.describe_feature(), to help explain what a feature is and the steps Featuretools took to generate it.\n",
    "feature = feature_names[54]\n",
    "ft.graph_feature(feature)\n",
    "\n",
    "\n",
    "# %%\n",
    "ft.describe_feature(feature)\n",
    "\n",
    "\n",
    "# %%\n",
    "feature_matrix,feature_names =ft.dfs(entityset = es, target_entity = 'WIP',\n",
    "                       trans_primitives=['weekday', 'day'],\n",
    "                       #agg_primitives=default_agg_primitives, \n",
    "                        verbose = 1,\n",
    "                       features_only = False)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# %%\n",
    "#累績總計 與總數\n",
    " \n",
    "feature_matrix, features_list = ft.dfs(entityset=es, \n",
    "    verbose = 1,\n",
    "\ttarget_entity='WIP',\n",
    "    agg_primitives=[], \n",
    "\ttrans_primitives=['cum_sum', 'cum_count'], \n",
    "\tgroupby_trans_primitives=['cum_sum', 'cum_count'], #累積總計、累積總數\n",
    "    \n",
    "    ignore_variables={'WIP': ['PRIORITY']}\n",
    "    \n",
    "\t)\n",
    "feature_matrix    \n",
    "\n",
    "\n",
    "# %%\n",
    "from featuretools.primitives import CumCount\n",
    "#累績總數\n",
    "cum_count = CumCount()\n",
    "cum_count([1, 2, 3, 4, None, 5]).tolist()\n",
    "\n",
    "\n",
    "# %%\n",
    "from featuretools.primitives import CumCount\n",
    "from featuretools.primitives import IsWeekend\n",
    "#累績總數\n",
    "cum_count = CumCount()\n",
    "cum_count([1, 2, 3, 4, None, 5]).tolist()\n",
    "isWkend = IsWeekend()\n",
    "feature_matrix, features_list = ft.dfs(entityset=es, \n",
    "    verbose = 1,\n",
    "\ttarget_entity='WIP',\n",
    "    agg_primitives=[], \n",
    "\ttrans_primitives=['cum_sum','cum_count','is_weekend'], \n",
    "\tprimitive_options={\n",
    "        'cum_sum': {\n",
    "                'include_variables':\n",
    "                    {'WIP': ['WIP_QTY']},\n",
    "                'ignore_variables':\n",
    "                    {'WIP': ['IS_MAIN_ROUTE']}    \n",
    "                    \n",
    "                    },\n",
    "\t\t'cum_count': {\n",
    "                'include_variables':\n",
    "                    {'WIP': ['WIP_QTY']},\n",
    "                'ignore_variables':\n",
    "                    {'WIP': ['IS_MAIN_ROUTE']}    \n",
    "                    \n",
    "                    },\n",
    "        'is_weekend': {\n",
    "                'ignore_variables':\n",
    "                        {'WIP': ['ACTUAL_WP_OUT']}}\n",
    "        }\n",
    "    \n",
    "\t)\n",
    "\n",
    "\n",
    "\n",
    "feature_matrix    \n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "from sklearn.svm import LinearSVC\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(train_X, train_y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(train_X)\n",
    "X_selected_df = pd.DataFrame(X_new, columns=[train_X.columns[i] for i in range(len(train_X.columns)) if model.get_support()[i]])\n",
    "X_selected_df.shape\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "random_forest = RandomForestClassifier(n_estimators=2000,oob_score=True)\n",
    "random_forest.fit(X_selected_df, train_y)\n",
    "\n",
    "\n",
    "# %%\n",
    "## 客制function\n",
    "\n",
    "\n",
    "# %%\n",
    "'''\n",
    "from featuretools.primitives import make_agg_primitive, make_trans_primitive\n",
    "from featuretools.variable_types import Text, Numeric\n",
    "\n",
    "def avg_goals_previous_n_games(home_team, away_team, home_goals, away_goals, which_team=None, n=1):\n",
    "    # make dataframe so it's easier to work with\n",
    "    df = pd.DataFrame({\n",
    "        \"home_team\": home_team,\n",
    "        \"away_team\": away_team,\n",
    "        \"home_goals\": home_goals,\n",
    "        \"away_goals\": away_goals\n",
    "        })\n",
    "\n",
    "    result = []\n",
    "    for i, current_game in df.iterrows():\n",
    "        # get the right team for this game\n",
    "        team = current_game[which_team]\n",
    "\n",
    "        # find all previous games that have been played\n",
    "        prev_games =  df.iloc[:i]\n",
    "\n",
    "        # only get games the team participated in\n",
    "        participated = prev_games[(prev_games[\"home_team\"] == team) | (prev_games[\"away_team\"] == team)]\n",
    "        if participated.shape[0] < n:\n",
    "            result.append(None)\n",
    "            continue\n",
    "\n",
    "        # get last n games\n",
    "        last_n = participated.tail(n)\n",
    "\n",
    "        # calculate games per game\n",
    "        goal_as_home = (last_n[\"home_team\"] == team) * last_n[\"home_goals\"]\n",
    "        goal_as_away = (last_n[\"away_team\"] == team) * last_n[\"away_goals\"]\n",
    "\n",
    "        # calculate mean across all home and away games\n",
    "        mean = (goal_as_home + goal_as_away).mean()\n",
    "\n",
    "        result.append(mean)\n",
    "\n",
    "    return result\n",
    "# custom function so the name of the feature prints out correctly\n",
    "def make_name(self):\n",
    "    return \"%s_goal_last_%d\" % (self.kwargs['which_team'], self.kwargs['n'])\n",
    "AvgGoalPreviousNGames = make_trans_primitive(function=avg_goals_previous_n_games,\n",
    "                                          input_types=[Categorical, Categorical, Numeric, Numeric],\n",
    "                                          return_type=Numeric,\n",
    "                                          cls_attributes={\"generate_name\": make_name, \"uses_full_entity\":True})\n",
    "\n",
    "input_vars = [es[\"matches\"][\"home_team\"], es[\"matches\"][\"away_team\"], es[\"matches\"][\"home_goals\"], es[\"matches\"][\"away_goals\"]]\n",
    "home_team_last1 = AvgGoalPreviousNGames(*input_vars, which_team=\"home_team\", n=1)\n",
    "home_team_last3 = AvgGoalPreviousNGames(*input_vars, which_team=\"home_team\", n=3)\n",
    "home_team_last5 = AvgGoalPreviousNGames(*input_vars, which_team=\"home_team\", n=5)\n",
    "away_team_last1 = AvgGoalPreviousNGames(*input_vars, which_team=\"away_team\", n=1)\n",
    "away_team_last3 = AvgGoalPreviousNGames(*input_vars, which_team=\"away_team\", n=3)\n",
    "away_team_last5 = AvgGoalPreviousNGames(*input_vars, which_team=\"away_team\", n=5)\n",
    "\n",
    "features = [home_team_last1, home_team_last3, home_team_last5,\n",
    "            away_team_last1, away_team_last3, away_team_last5]\n",
    "\n",
    "\n",
    "fm = ft.calculate_feature_matrix(entityset=es, features=features) \n",
    "\n",
    "fm, feature_defs = ft.dfs(entityset=es, \n",
    "                          target_entity=\"matches\",\n",
    "                          seed_features=features, \n",
    "                          agg_primitives=[], \n",
    "                          trans_primitives=[\"day\", \"month\", \"year\", \"weekday\", \"percentile\"])\n",
    "\n",
    "#https://stackoverflow.com/questions/53579465/how-to-use-featuretools-to-create-features-from-multiple-columns-in-single-dataf                          \n",
    "\n",
    "\n"
   ]
  }
 ]
}