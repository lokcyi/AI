{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0fd79c5f311e48de877911d8cf1d56e7b724990a605ba536e714d512526dbebf5",
   "display_name": "Python 3.7.9 64-bit ('PY379': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "configpartno='85-EKA0190'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrain():\n",
    "    train =  pd.read_csv(\"./data/Parts_EQP_Output_ByMonth_20210407_van.csv\")\n",
    "    train= train[train['PART_NO']==configpartno]  \n",
    "    train.drop(columns=['PART_NO','EQP_NO','MFG_MONTH','PM','TS','ENG','NST'],inplace=True)\n",
    "    train.groupby(['STOCK_EVENT_TIME']).sum().reset_index()\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = readTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   STOCK_EVENT_TIME  QTY\n",
       "0        2015-01-31    0\n",
       "1        2015-02-28    0\n",
       "2        2015-03-31    0\n",
       "3        2015-04-30    0\n",
       "4        2015-05-31    0\n",
       "..              ...  ...\n",
       "70       2020-11-30  136\n",
       "71       2020-12-31  156\n",
       "72       2021-01-31  150\n",
       "73       2021-02-28  117\n",
       "74       2021-03-31  122\n",
       "\n",
       "[75 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STOCK_EVENT_TIME</th>\n      <th>QTY</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-01-31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-02-28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-03-31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-04-30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-05-31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>2020-11-30</td>\n      <td>136</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>2020-12-31</td>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>2021-01-31</td>\n      <td>150</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>2021-02-28</td>\n      <td>117</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>2021-03-31</td>\n      <td>122</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augFeatures(train):\n",
    "  train[\"Date\"] = pd.to_datetime(train[\"STOCK_EVENT_TIME\"])\n",
    "  train[\"year\"] = train[\"Date\"].dt.year\n",
    "  train[\"month\"] = train[\"Date\"].dt.month\n",
    "#   train[\"date\"] = train[\"Date\"].dt.day\n",
    "#   train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "  return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(train):\n",
    "    train.drop(columns=[\"Date\",\"STOCK_EVENT_TIME\"], axis=1,inplace=True)\n",
    "    # train = train.drop([\"Date\"], axis=1)\n",
    "    train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildTrain(train, pastDay=30, futureDay=5):\n",
    "  X_train, Y_train = [], []\n",
    "  for i in range(train.shape[0]-futureDay-pastDay):\n",
    "    X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "    Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"QTY\"]))\n",
    "  return np.array(X_train), np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(X,Y):\n",
    "  np.random.seed(10)\n",
    "  randomList = np.arange(X.shape[0])\n",
    "  np.random.shuffle(randomList)\n",
    "  return X[randomList], Y[randomList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def splitData(X,Y,rate):\n",
    "  X_train = X[int(X.shape[0]*rate):]\n",
    "  Y_train = Y[int(Y.shape[0]*rate):]\n",
    "  X_val = X[:int(X.shape[0]*rate)]\n",
    "  Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "  return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read SPY.csv\n",
    "train = readTrain()\n",
    "\n",
    "# Augment the features (year, month, date, day)\n",
    "train_Aug = augFeatures(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     STOCK_EVENT_TIME  QTY       Date  year  month\n",
       "3150       2015-01-31    0 2015-01-31  2015      1\n",
       "3151       2015-02-28    0 2015-02-28  2015      2\n",
       "3152       2015-03-31    0 2015-03-31  2015      3\n",
       "3153       2015-04-30    0 2015-04-30  2015      4\n",
       "3154       2015-05-31    0 2015-05-31  2015      5\n",
       "...               ...  ...        ...   ...    ...\n",
       "3745       2020-11-30   15 2020-11-30  2020     11\n",
       "3746       2020-12-31   20 2020-12-31  2020     12\n",
       "3747       2021-01-31   15 2021-01-31  2021      1\n",
       "3748       2021-02-28    5 2021-02-28  2021      2\n",
       "3749       2021-03-31   15 2021-03-31  2021      3\n",
       "\n",
       "[600 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>STOCK_EVENT_TIME</th>\n      <th>QTY</th>\n      <th>Date</th>\n      <th>year</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3150</th>\n      <td>2015-01-31</td>\n      <td>0</td>\n      <td>2015-01-31</td>\n      <td>2015</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3151</th>\n      <td>2015-02-28</td>\n      <td>0</td>\n      <td>2015-02-28</td>\n      <td>2015</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3152</th>\n      <td>2015-03-31</td>\n      <td>0</td>\n      <td>2015-03-31</td>\n      <td>2015</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3153</th>\n      <td>2015-04-30</td>\n      <td>0</td>\n      <td>2015-04-30</td>\n      <td>2015</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3154</th>\n      <td>2015-05-31</td>\n      <td>0</td>\n      <td>2015-05-31</td>\n      <td>2015</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3745</th>\n      <td>2020-11-30</td>\n      <td>15</td>\n      <td>2020-11-30</td>\n      <td>2020</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>3746</th>\n      <td>2020-12-31</td>\n      <td>20</td>\n      <td>2020-12-31</td>\n      <td>2020</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>3747</th>\n      <td>2021-01-31</td>\n      <td>15</td>\n      <td>2021-01-31</td>\n      <td>2021</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3748</th>\n      <td>2021-02-28</td>\n      <td>5</td>\n      <td>2021-02-28</td>\n      <td>2021</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3749</th>\n      <td>2021-03-31</td>\n      <td>15</td>\n      <td>2021-03-31</td>\n      <td>2021</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "train_Aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "train_norm = normalize(train_Aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build Data, use last 30 days to predict next 5 days\n",
    "X_train, Y_train = buildTrain(train_norm, 30, 5)\n",
    "\n",
    "# shuffle the data, and random seed is 10\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "\n",
    "# split training data and validation data\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "# X_trian: (5710, 30, 10)\n",
    "# Y_train: (5710, 5, 1)\n",
    "# X_val: (634, 30, 10)\n",
    "# Y_val: (634, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToOneModel(shape):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add( tf.keras.layers.LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)))    # or use model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.Adam())\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # model = tf.keras.Sequential([\n",
    "    #         tf.keras.layers.Dense(units=32,activation = 'relu',input_shape=[X.shape[1]]),\n",
    "    #         tf.keras.layers.Dense(units=16,activation = 'relu'),\n",
    "    #         tf.keras.layers.Dense(units=4,activation = 'relu'), \n",
    "    #         tf.keras.layers.Dense(units=1)\n",
    "    #         ]) \n",
    "    #     model.compile(loss='mean_squared_error', \n",
    "    #         optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    #         metrics=[tf.keras.metrics.MeanAbsoluteError()]) \n",
    "\n",
    "    #     history = model.fit(X, y, epochs=500, batch_size=16, verbose=True,\n",
    "    #         validation_split=0.01)   \n",
    "    #     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 10)             560       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 571\n",
      "Trainable params: 571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 539 samples, validate on 59 samples\n",
      "Epoch 1/1000\n",
      "539/539 [==============================] - 4s 7ms/sample - loss: 0.0752 - val_loss: 0.0639\n",
      "Epoch 2/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0740 - val_loss: 0.0631\n",
      "Epoch 3/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0728 - val_loss: 0.0623\n",
      "Epoch 4/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0718 - val_loss: 0.0615\n",
      "Epoch 5/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0707 - val_loss: 0.0608\n",
      "Epoch 6/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0698 - val_loss: 0.0601\n",
      "Epoch 7/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0688 - val_loss: 0.0595\n",
      "Epoch 8/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0679 - val_loss: 0.0590\n",
      "Epoch 9/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0670 - val_loss: 0.0584\n",
      "Epoch 10/1000\n",
      "539/539 [==============================] - 0s 72us/sample - loss: 0.0661 - val_loss: 0.0577\n",
      "Epoch 11/1000\n",
      "539/539 [==============================] - 0s 87us/sample - loss: 0.0653 - val_loss: 0.0571\n",
      "Epoch 12/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0645 - val_loss: 0.0565\n",
      "Epoch 13/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0638 - val_loss: 0.0559\n",
      "Epoch 14/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0631 - val_loss: 0.0553\n",
      "Epoch 15/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0624 - val_loss: 0.0547\n",
      "Epoch 16/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0617 - val_loss: 0.0542\n",
      "Epoch 17/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0610 - val_loss: 0.0537\n",
      "Epoch 18/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0604 - val_loss: 0.0533\n",
      "Epoch 19/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0597 - val_loss: 0.0529\n",
      "Epoch 20/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0591 - val_loss: 0.0525\n",
      "Epoch 21/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0585 - val_loss: 0.0520\n",
      "Epoch 22/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0579 - val_loss: 0.0517\n",
      "Epoch 23/1000\n",
      "539/539 [==============================] - 0s 65us/sample - loss: 0.0573 - val_loss: 0.0513\n",
      "Epoch 24/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0567 - val_loss: 0.0510\n",
      "Epoch 25/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0561 - val_loss: 0.0506\n",
      "Epoch 26/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0556 - val_loss: 0.0501\n",
      "Epoch 27/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0550 - val_loss: 0.0497\n",
      "Epoch 28/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0544 - val_loss: 0.0493\n",
      "Epoch 29/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0538 - val_loss: 0.0488\n",
      "Epoch 30/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0533 - val_loss: 0.0484\n",
      "Epoch 31/1000\n",
      "539/539 [==============================] - 0s 69us/sample - loss: 0.0528 - val_loss: 0.0479\n",
      "Epoch 32/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0522 - val_loss: 0.0475\n",
      "Epoch 33/1000\n",
      "539/539 [==============================] - 0s 63us/sample - loss: 0.0516 - val_loss: 0.0471\n",
      "Epoch 34/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0510 - val_loss: 0.0468\n",
      "Epoch 35/1000\n",
      "539/539 [==============================] - 0s 63us/sample - loss: 0.0504 - val_loss: 0.0464\n",
      "Epoch 36/1000\n",
      "539/539 [==============================] - 0s 65us/sample - loss: 0.0499 - val_loss: 0.0461\n",
      "Epoch 37/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0493 - val_loss: 0.0457\n",
      "Epoch 38/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0487 - val_loss: 0.0453\n",
      "Epoch 39/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0481 - val_loss: 0.0447\n",
      "Epoch 40/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0475 - val_loss: 0.0441\n",
      "Epoch 41/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0469 - val_loss: 0.0436\n",
      "Epoch 42/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0462 - val_loss: 0.0430\n",
      "Epoch 43/1000\n",
      "539/539 [==============================] - 0s 70us/sample - loss: 0.0456 - val_loss: 0.0424\n",
      "Epoch 44/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0449 - val_loss: 0.0419\n",
      "Epoch 45/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0443 - val_loss: 0.0414\n",
      "Epoch 46/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0436 - val_loss: 0.0407\n",
      "Epoch 47/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0429 - val_loss: 0.0401\n",
      "Epoch 48/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0422 - val_loss: 0.0395\n",
      "Epoch 49/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0416 - val_loss: 0.0389\n",
      "Epoch 50/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0409 - val_loss: 0.0383\n",
      "Epoch 51/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0401 - val_loss: 0.0377\n",
      "Epoch 52/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0395 - val_loss: 0.0371\n",
      "Epoch 53/1000\n",
      "539/539 [==============================] - 0s 65us/sample - loss: 0.0387 - val_loss: 0.0365\n",
      "Epoch 54/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0380 - val_loss: 0.0358\n",
      "Epoch 55/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0372 - val_loss: 0.0352\n",
      "Epoch 56/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0365 - val_loss: 0.0346\n",
      "Epoch 57/1000\n",
      "539/539 [==============================] - 0s 169us/sample - loss: 0.0357 - val_loss: 0.0339\n",
      "Epoch 58/1000\n",
      "539/539 [==============================] - 0s 74us/sample - loss: 0.0349 - val_loss: 0.0333\n",
      "Epoch 59/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0341 - val_loss: 0.0327\n",
      "Epoch 60/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0333 - val_loss: 0.0320\n",
      "Epoch 61/1000\n",
      "539/539 [==============================] - 0s 91us/sample - loss: 0.0325 - val_loss: 0.0312\n",
      "Epoch 62/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0316 - val_loss: 0.0305\n",
      "Epoch 63/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0308 - val_loss: 0.0298\n",
      "Epoch 64/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0300 - val_loss: 0.0291\n",
      "Epoch 65/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0292 - val_loss: 0.0283\n",
      "Epoch 66/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0284 - val_loss: 0.0275\n",
      "Epoch 67/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0276 - val_loss: 0.0268\n",
      "Epoch 68/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0268 - val_loss: 0.0261\n",
      "Epoch 69/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0260 - val_loss: 0.0254\n",
      "Epoch 70/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0252 - val_loss: 0.0247\n",
      "Epoch 71/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0245 - val_loss: 0.0241\n",
      "Epoch 72/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0237 - val_loss: 0.0235\n",
      "Epoch 73/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0230 - val_loss: 0.0229\n",
      "Epoch 74/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0222 - val_loss: 0.0224\n",
      "Epoch 75/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0215 - val_loss: 0.0219\n",
      "Epoch 76/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0208 - val_loss: 0.0214\n",
      "Epoch 77/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0201 - val_loss: 0.0209\n",
      "Epoch 78/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0195 - val_loss: 0.0203\n",
      "Epoch 79/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0189 - val_loss: 0.0198\n",
      "Epoch 80/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0183 - val_loss: 0.0192\n",
      "Epoch 81/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0177 - val_loss: 0.0187\n",
      "Epoch 82/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0171 - val_loss: 0.0182\n",
      "Epoch 83/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0166 - val_loss: 0.0178\n",
      "Epoch 84/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0161 - val_loss: 0.0175\n",
      "Epoch 85/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0157 - val_loss: 0.0172\n",
      "Epoch 86/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0152 - val_loss: 0.0170\n",
      "Epoch 87/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0148 - val_loss: 0.0167\n",
      "Epoch 88/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0144 - val_loss: 0.0165\n",
      "Epoch 89/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0141 - val_loss: 0.0163\n",
      "Epoch 90/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0138 - val_loss: 0.0161\n",
      "Epoch 91/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0135 - val_loss: 0.0159\n",
      "Epoch 92/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0132 - val_loss: 0.0158\n",
      "Epoch 93/1000\n",
      "539/539 [==============================] - 0s 57us/sample - loss: 0.0130 - val_loss: 0.0156\n",
      "Epoch 94/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0128 - val_loss: 0.0154\n",
      "Epoch 95/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0126 - val_loss: 0.0153\n",
      "Epoch 96/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0124 - val_loss: 0.0152\n",
      "Epoch 97/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0122 - val_loss: 0.0151\n",
      "Epoch 98/1000\n",
      "539/539 [==============================] - 0s 45us/sample - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 99/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0120 - val_loss: 0.0148\n",
      "Epoch 100/1000\n",
      "539/539 [==============================] - 0s 63us/sample - loss: 0.0119 - val_loss: 0.0147\n",
      "Epoch 101/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0118 - val_loss: 0.0147\n",
      "Epoch 102/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0117 - val_loss: 0.0147\n",
      "Epoch 103/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0116 - val_loss: 0.0148\n",
      "Epoch 104/1000\n",
      "539/539 [==============================] - 0s 59us/sample - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 105/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0115 - val_loss: 0.0149\n",
      "Epoch 106/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 107/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 108/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0114 - val_loss: 0.0148\n",
      "Epoch 109/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0113 - val_loss: 0.0148\n",
      "Epoch 110/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 111/1000\n",
      "539/539 [==============================] - 0s 147us/sample - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 112/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 113/1000\n",
      "539/539 [==============================] - 0s 72us/sample - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 114/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0113 - val_loss: 0.0150\n",
      "Epoch 115/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0113 - val_loss: 0.0149\n",
      "Epoch 116/1000\n",
      "539/539 [==============================] - 0s 67us/sample - loss: 0.0113 - val_loss: 0.0148\n",
      "Epoch 117/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 118/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 119/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 120/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 121/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 122/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 123/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 124/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 125/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 126/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 127/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 128/1000\n",
      "539/539 [==============================] - 0s 72us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 129/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 130/1000\n",
      "539/539 [==============================] - 0s 58us/sample - loss: 0.0112 - val_loss: 0.0150\n",
      "Epoch 131/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 132/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0112 - val_loss: 0.0151\n",
      "Epoch 133/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0150\n",
      "Epoch 134/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 135/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 136/1000\n",
      "539/539 [==============================] - 0s 69us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 137/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 138/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 139/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 140/1000\n",
      "539/539 [==============================] - 0s 61us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 141/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 142/1000\n",
      "539/539 [==============================] - 0s 48us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 143/1000\n",
      "539/539 [==============================] - 0s 46us/sample - loss: 0.0112 - val_loss: 0.0148\n",
      "Epoch 144/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 145/1000\n",
      "539/539 [==============================] - 0s 52us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 146/1000\n",
      "539/539 [==============================] - 0s 50us/sample - loss: 0.0112 - val_loss: 0.0150\n",
      "Epoch 147/1000\n",
      "539/539 [==============================] - 0s 54us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 148/1000\n",
      "539/539 [==============================] - 0s 56us/sample - loss: 0.0112 - val_loss: 0.0149\n",
      "Epoch 00148: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e20086d48>"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多對一模型\n",
    "def buildManyToOneModel(shape):\n",
    "#   model = Sequential()\n",
    "#   model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "#   # output shape: (1, 1)\n",
    "#   model.add(Dense(1))\n",
    "#   model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "#   model.summary()\n",
    "#   return model\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(units=10, input_length=shape[1], input_dim=shape[2]),\n",
    "            tf.keras.layers.Dense(units=1),\n",
    "            ]) \n",
    "    model.compile(loss='mean_squared_error', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            # metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "            ) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           QTY      year     month\n",
       "3150 -0.268022 -0.440000 -0.483636\n",
       "3151 -0.268022 -0.440000 -0.392727\n",
       "3152 -0.268022 -0.440000 -0.301818\n",
       "3153 -0.268022 -0.440000 -0.210909\n",
       "3154 -0.268022 -0.440000 -0.120000\n",
       "...        ...       ...       ...\n",
       "3745 -0.068022  0.393333  0.425455\n",
       "3746 -0.001356  0.393333  0.516364\n",
       "3747 -0.068022  0.560000 -0.483636\n",
       "3748 -0.201356  0.560000 -0.392727\n",
       "3749 -0.068022  0.560000 -0.301818\n",
       "\n",
       "[600 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>QTY</th>\n      <th>year</th>\n      <th>month</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3150</th>\n      <td>-0.268022</td>\n      <td>-0.440000</td>\n      <td>-0.483636</td>\n    </tr>\n    <tr>\n      <th>3151</th>\n      <td>-0.268022</td>\n      <td>-0.440000</td>\n      <td>-0.392727</td>\n    </tr>\n    <tr>\n      <th>3152</th>\n      <td>-0.268022</td>\n      <td>-0.440000</td>\n      <td>-0.301818</td>\n    </tr>\n    <tr>\n      <th>3153</th>\n      <td>-0.268022</td>\n      <td>-0.440000</td>\n      <td>-0.210909</td>\n    </tr>\n    <tr>\n      <th>3154</th>\n      <td>-0.268022</td>\n      <td>-0.440000</td>\n      <td>-0.120000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3745</th>\n      <td>-0.068022</td>\n      <td>0.393333</td>\n      <td>0.425455</td>\n    </tr>\n    <tr>\n      <th>3746</th>\n      <td>-0.001356</td>\n      <td>0.393333</td>\n      <td>0.516364</td>\n    </tr>\n    <tr>\n      <th>3747</th>\n      <td>-0.068022</td>\n      <td>0.560000</td>\n      <td>-0.483636</td>\n    </tr>\n    <tr>\n      <th>3748</th>\n      <td>-0.201356</td>\n      <td>0.560000</td>\n      <td>-0.392727</td>\n    </tr>\n    <tr>\n      <th>3749</th>\n      <td>-0.068022</td>\n      <td>0.560000</td>\n      <td>-0.301818</td>\n    </tr>\n  </tbody>\n</table>\n<p>600 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "train_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 10)                560       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 571\n",
      "Trainable params: 571\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 513 samples, validate on 56 samples\n",
      "Epoch 1/1000\n",
      "513/513 [==============================] - 4s 7ms/sample - loss: 0.0555 - val_loss: 0.0704\n",
      "Epoch 2/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0502 - val_loss: 0.0640\n",
      "Epoch 3/1000\n",
      "513/513 [==============================] - 0s 154us/sample - loss: 0.0462 - val_loss: 0.0589\n",
      "Epoch 4/1000\n",
      "513/513 [==============================] - 0s 154us/sample - loss: 0.0424 - val_loss: 0.0545\n",
      "Epoch 5/1000\n",
      "513/513 [==============================] - 0s 168us/sample - loss: 0.0389 - val_loss: 0.0510\n",
      "Epoch 6/1000\n",
      "513/513 [==============================] - 0s 181us/sample - loss: 0.0360 - val_loss: 0.0474\n",
      "Epoch 7/1000\n",
      "513/513 [==============================] - 0s 175us/sample - loss: 0.0334 - val_loss: 0.0437\n",
      "Epoch 8/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0311 - val_loss: 0.0404\n",
      "Epoch 9/1000\n",
      "513/513 [==============================] - 0s 164us/sample - loss: 0.0290 - val_loss: 0.0372\n",
      "Epoch 10/1000\n",
      "513/513 [==============================] - 0s 203us/sample - loss: 0.0268 - val_loss: 0.0335\n",
      "Epoch 11/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0246 - val_loss: 0.0304\n",
      "Epoch 12/1000\n",
      "513/513 [==============================] - 0s 162us/sample - loss: 0.0227 - val_loss: 0.0282\n",
      "Epoch 13/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0209 - val_loss: 0.0268\n",
      "Epoch 14/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0199 - val_loss: 0.0264\n",
      "Epoch 15/1000\n",
      "513/513 [==============================] - 0s 209us/sample - loss: 0.0194 - val_loss: 0.0254\n",
      "Epoch 16/1000\n",
      "513/513 [==============================] - 0s 162us/sample - loss: 0.0187 - val_loss: 0.0235\n",
      "Epoch 17/1000\n",
      "513/513 [==============================] - 0s 218us/sample - loss: 0.0180 - val_loss: 0.0223\n",
      "Epoch 18/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0178 - val_loss: 0.0215\n",
      "Epoch 19/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0176 - val_loss: 0.0209\n",
      "Epoch 20/1000\n",
      "513/513 [==============================] - 0s 193us/sample - loss: 0.0173 - val_loss: 0.0206\n",
      "Epoch 21/1000\n",
      "513/513 [==============================] - 0s 189us/sample - loss: 0.0171 - val_loss: 0.0202\n",
      "Epoch 22/1000\n",
      "513/513 [==============================] - 0s 168us/sample - loss: 0.0169 - val_loss: 0.0197\n",
      "Epoch 23/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0171 - val_loss: 0.0202\n",
      "Epoch 24/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0177 - val_loss: 0.0204\n",
      "Epoch 25/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0178 - val_loss: 0.0202\n",
      "Epoch 26/1000\n",
      "513/513 [==============================] - 0s 172us/sample - loss: 0.0181 - val_loss: 0.0201\n",
      "Epoch 27/1000\n",
      "513/513 [==============================] - 0s 168us/sample - loss: 0.0188 - val_loss: 0.0191\n",
      "Epoch 28/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0189 - val_loss: 0.0178\n",
      "Epoch 29/1000\n",
      "513/513 [==============================] - 0s 366us/sample - loss: 0.0183 - val_loss: 0.0168\n",
      "Epoch 30/1000\n",
      "513/513 [==============================] - 0s 170us/sample - loss: 0.0176 - val_loss: 0.0159\n",
      "Epoch 31/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0167 - val_loss: 0.0155\n",
      "Epoch 32/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 33/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0157 - val_loss: 0.0158\n",
      "Epoch 34/1000\n",
      "513/513 [==============================] - 0s 218us/sample - loss: 0.0155 - val_loss: 0.0160\n",
      "Epoch 35/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0154 - val_loss: 0.0162\n",
      "Epoch 36/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0153 - val_loss: 0.0164\n",
      "Epoch 37/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0153 - val_loss: 0.0167\n",
      "Epoch 38/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0154 - val_loss: 0.0169\n",
      "Epoch 39/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0154 - val_loss: 0.0170\n",
      "Epoch 40/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0150 - val_loss: 0.0173\n",
      "Epoch 41/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0149 - val_loss: 0.0176\n",
      "Epoch 42/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0149 - val_loss: 0.0174\n",
      "Epoch 43/1000\n",
      "513/513 [==============================] - 0s 134us/sample - loss: 0.0148 - val_loss: 0.0171\n",
      "Epoch 44/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0147 - val_loss: 0.0168\n",
      "Epoch 45/1000\n",
      "513/513 [==============================] - 0s 152us/sample - loss: 0.0147 - val_loss: 0.0166\n",
      "Epoch 46/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0146 - val_loss: 0.0165\n",
      "Epoch 47/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0146 - val_loss: 0.0164\n",
      "Epoch 48/1000\n",
      "513/513 [==============================] - 0s 138us/sample - loss: 0.0145 - val_loss: 0.0165\n",
      "Epoch 49/1000\n",
      "513/513 [==============================] - 0s 138us/sample - loss: 0.0145 - val_loss: 0.0166\n",
      "Epoch 50/1000\n",
      "513/513 [==============================] - 0s 156us/sample - loss: 0.0145 - val_loss: 0.0168\n",
      "Epoch 51/1000\n",
      "513/513 [==============================] - 0s 138us/sample - loss: 0.0145 - val_loss: 0.0168\n",
      "Epoch 52/1000\n",
      "513/513 [==============================] - 0s 162us/sample - loss: 0.0144 - val_loss: 0.0166\n",
      "Epoch 53/1000\n",
      "513/513 [==============================] - 0s 154us/sample - loss: 0.0143 - val_loss: 0.0166\n",
      "Epoch 54/1000\n",
      "513/513 [==============================] - 0s 148us/sample - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 55/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0143 - val_loss: 0.0173\n",
      "Epoch 56/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0144 - val_loss: 0.0175\n",
      "Epoch 57/1000\n",
      "513/513 [==============================] - 0s 160us/sample - loss: 0.0144 - val_loss: 0.0173\n",
      "Epoch 58/1000\n",
      "513/513 [==============================] - 0s 129us/sample - loss: 0.0142 - val_loss: 0.0170\n",
      "Epoch 59/1000\n",
      "513/513 [==============================] - 0s 131us/sample - loss: 0.0140 - val_loss: 0.0170\n",
      "Epoch 60/1000\n",
      "513/513 [==============================] - 0s 129us/sample - loss: 0.0140 - val_loss: 0.0171\n",
      "Epoch 61/1000\n",
      "513/513 [==============================] - 0s 131us/sample - loss: 0.0140 - val_loss: 0.0171\n",
      "Epoch 62/1000\n",
      "513/513 [==============================] - 0s 185us/sample - loss: 0.0139 - val_loss: 0.0169\n",
      "Epoch 63/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0138 - val_loss: 0.0167\n",
      "Epoch 64/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0137 - val_loss: 0.0164\n",
      "Epoch 65/1000\n",
      "513/513 [==============================] - 0s 181us/sample - loss: 0.0136 - val_loss: 0.0159\n",
      "Epoch 66/1000\n",
      "513/513 [==============================] - 0s 166us/sample - loss: 0.0137 - val_loss: 0.0158\n",
      "Epoch 67/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0138 - val_loss: 0.0158\n",
      "Epoch 68/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0137 - val_loss: 0.0157\n",
      "Epoch 69/1000\n",
      "513/513 [==============================] - 0s 154us/sample - loss: 0.0136 - val_loss: 0.0157\n",
      "Epoch 70/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0134 - val_loss: 0.0156\n",
      "Epoch 71/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0134 - val_loss: 0.0156\n",
      "Epoch 72/1000\n",
      "513/513 [==============================] - 0s 129us/sample - loss: 0.0134 - val_loss: 0.0156\n",
      "Epoch 73/1000\n",
      "513/513 [==============================] - 0s 133us/sample - loss: 0.0134 - val_loss: 0.0155\n",
      "Epoch 74/1000\n",
      "513/513 [==============================] - 0s 222us/sample - loss: 0.0134 - val_loss: 0.0155\n",
      "Epoch 75/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0133 - val_loss: 0.0154\n",
      "Epoch 76/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0135 - val_loss: 0.0157\n",
      "Epoch 77/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0137 - val_loss: 0.0156\n",
      "Epoch 78/1000\n",
      "513/513 [==============================] - 0s 168us/sample - loss: 0.0135 - val_loss: 0.0154\n",
      "Epoch 79/1000\n",
      "513/513 [==============================] - 0s 197us/sample - loss: 0.0133 - val_loss: 0.0152\n",
      "Epoch 80/1000\n",
      "513/513 [==============================] - 0s 148us/sample - loss: 0.0132 - val_loss: 0.0150\n",
      "Epoch 81/1000\n",
      "513/513 [==============================] - 0s 158us/sample - loss: 0.0132 - val_loss: 0.0148\n",
      "Epoch 82/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0131 - val_loss: 0.0148\n",
      "Epoch 83/1000\n",
      "513/513 [==============================] - 0s 193us/sample - loss: 0.0131 - val_loss: 0.0147\n",
      "Epoch 84/1000\n",
      "513/513 [==============================] - 0s 152us/sample - loss: 0.0130 - val_loss: 0.0146\n",
      "Epoch 85/1000\n",
      "513/513 [==============================] - 0s 212us/sample - loss: 0.0129 - val_loss: 0.0145\n",
      "Epoch 86/1000\n",
      "513/513 [==============================] - 0s 179us/sample - loss: 0.0128 - val_loss: 0.0144\n",
      "Epoch 87/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0128 - val_loss: 0.0144\n",
      "Epoch 88/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0128 - val_loss: 0.0144\n",
      "Epoch 89/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0127 - val_loss: 0.0145\n",
      "Epoch 90/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 91/1000\n",
      "513/513 [==============================] - 0s 189us/sample - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 92/1000\n",
      "513/513 [==============================] - 0s 179us/sample - loss: 0.0127 - val_loss: 0.0146\n",
      "Epoch 93/1000\n",
      "513/513 [==============================] - 0s 152us/sample - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 94/1000\n",
      "513/513 [==============================] - 0s 144us/sample - loss: 0.0127 - val_loss: 0.0147\n",
      "Epoch 95/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0126 - val_loss: 0.0147\n",
      "Epoch 96/1000\n",
      "513/513 [==============================] - 0s 148us/sample - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 97/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0126 - val_loss: 0.0152\n",
      "Epoch 98/1000\n",
      "513/513 [==============================] - 0s 179us/sample - loss: 0.0126 - val_loss: 0.0156\n",
      "Epoch 99/1000\n",
      "513/513 [==============================] - 0s 172us/sample - loss: 0.0127 - val_loss: 0.0156\n",
      "Epoch 100/1000\n",
      "513/513 [==============================] - 0s 166us/sample - loss: 0.0126 - val_loss: 0.0157\n",
      "Epoch 101/1000\n",
      "513/513 [==============================] - 0s 154us/sample - loss: 0.0126 - val_loss: 0.0158\n",
      "Epoch 102/1000\n",
      "513/513 [==============================] - 0s 175us/sample - loss: 0.0126 - val_loss: 0.0158\n",
      "Epoch 103/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0125 - val_loss: 0.0156\n",
      "Epoch 104/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0125 - val_loss: 0.0152\n",
      "Epoch 105/1000\n",
      "513/513 [==============================] - 0s 134us/sample - loss: 0.0124 - val_loss: 0.0150\n",
      "Epoch 106/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0124 - val_loss: 0.0150\n",
      "Epoch 107/1000\n",
      "513/513 [==============================] - 0s 142us/sample - loss: 0.0124 - val_loss: 0.0148\n",
      "Epoch 108/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0123 - val_loss: 0.0145\n",
      "Epoch 109/1000\n",
      "513/513 [==============================] - 0s 148us/sample - loss: 0.0123 - val_loss: 0.0142\n",
      "Epoch 110/1000\n",
      "513/513 [==============================] - 0s 211us/sample - loss: 0.0122 - val_loss: 0.0142\n",
      "Epoch 111/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0122 - val_loss: 0.0142\n",
      "Epoch 112/1000\n",
      "513/513 [==============================] - 0s 162us/sample - loss: 0.0122 - val_loss: 0.0142\n",
      "Epoch 113/1000\n",
      "513/513 [==============================] - 0s 181us/sample - loss: 0.0121 - val_loss: 0.0141\n",
      "Epoch 114/1000\n",
      "513/513 [==============================] - 0s 148us/sample - loss: 0.0121 - val_loss: 0.0141\n",
      "Epoch 115/1000\n",
      "513/513 [==============================] - 0s 189us/sample - loss: 0.0121 - val_loss: 0.0144\n",
      "Epoch 116/1000\n",
      "513/513 [==============================] - 0s 133us/sample - loss: 0.0121 - val_loss: 0.0146\n",
      "Epoch 117/1000\n",
      "513/513 [==============================] - 0s 131us/sample - loss: 0.0122 - val_loss: 0.0147\n",
      "Epoch 118/1000\n",
      "513/513 [==============================] - 0s 134us/sample - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 119/1000\n",
      "513/513 [==============================] - 0s 131us/sample - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 120/1000\n",
      "513/513 [==============================] - 0s 136us/sample - loss: 0.0121 - val_loss: 0.0149\n",
      "Epoch 121/1000\n",
      "513/513 [==============================] - 0s 140us/sample - loss: 0.0121 - val_loss: 0.0145\n",
      "Epoch 122/1000\n",
      "513/513 [==============================] - 0s 127us/sample - loss: 0.0120 - val_loss: 0.0143\n",
      "Epoch 123/1000\n",
      "513/513 [==============================] - 0s 134us/sample - loss: 0.0120 - val_loss: 0.0142\n",
      "Epoch 124/1000\n",
      "513/513 [==============================] - 0s 129us/sample - loss: 0.0120 - val_loss: 0.0143\n",
      "Epoch 125/1000\n",
      "513/513 [==============================] - 0s 129us/sample - loss: 0.0122 - val_loss: 0.0148\n",
      "Epoch 126/1000\n",
      "513/513 [==============================] - 0s 134us/sample - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 127/1000\n",
      "513/513 [==============================] - 0s 131us/sample - loss: 0.0126 - val_loss: 0.0144\n",
      "Epoch 128/1000\n",
      "513/513 [==============================] - 0s 168us/sample - loss: 0.0122 - val_loss: 0.0139\n",
      "Epoch 129/1000\n",
      "513/513 [==============================] - 0s 150us/sample - loss: 0.0120 - val_loss: 0.0137\n",
      "Epoch 130/1000\n",
      "513/513 [==============================] - 0s 146us/sample - loss: 0.0119 - val_loss: 0.0135\n",
      "Epoch 131/1000\n",
      "513/513 [==============================] - 0s 138us/sample - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 132/1000\n",
      "513/513 [==============================] - 0s 183us/sample - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 133/1000\n",
      "513/513 [==============================] - 0s 185us/sample - loss: 0.0119 - val_loss: 0.0133\n",
      "Epoch 134/1000\n",
      "513/513 [==============================] - 0s 152us/sample - loss: 0.0119 - val_loss: 0.0134\n",
      "Epoch 135/1000\n",
      "513/513 [==============================] - 0s 173us/sample - loss: 0.0121 - val_loss: 0.0137\n",
      "Epoch 136/1000\n",
      "513/513 [==============================] - 0s 166us/sample - loss: 0.0122 - val_loss: 0.0139\n",
      "Epoch 137/1000\n",
      "513/513 [==============================] - 0s 185us/sample - loss: 0.0121 - val_loss: 0.0143\n",
      "Epoch 138/1000\n",
      "513/513 [==============================] - 0s 230us/sample - loss: 0.0120 - val_loss: 0.0145\n",
      "Epoch 139/1000\n",
      "513/513 [==============================] - 0s 193us/sample - loss: 0.0119 - val_loss: 0.0145\n",
      "Epoch 140/1000\n",
      "513/513 [==============================] - 0s 166us/sample - loss: 0.0119 - val_loss: 0.0145\n",
      "Epoch 00140: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e24628688>"
      ]
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildOneToManyModel(shape):\n",
    "#   model = Sequential()\n",
    "#   model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "#   # output shape: (5, 1)\n",
    "#   model.add(Dense(1))\n",
    "#   model.add(RepeatVector(5))\n",
    "#   model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "#   model.summary()\n",
    "#   return model\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(units=10, input_length=shape[1], input_dim=shape[2]),\n",
    "            tf.keras.layers.Dense(units=1),\n",
    "            tf.keras.layers.RepeatVector(5),\n",
    "            ]) \n",
    "    model.compile(loss='mean_squared_error', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            # metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "            ) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0243 - val_loss: 0.0190\n",
      "Epoch 44/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0239 - val_loss: 0.0186\n",
      "Epoch 45/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 46/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0232 - val_loss: 0.0180\n",
      "Epoch 47/1000\n",
      "535/535 [==============================] - 0s 84us/sample - loss: 0.0228 - val_loss: 0.0177\n",
      "Epoch 48/1000\n",
      "535/535 [==============================] - 0s 105us/sample - loss: 0.0225 - val_loss: 0.0174\n",
      "Epoch 49/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0222 - val_loss: 0.0172\n",
      "Epoch 50/1000\n",
      "535/535 [==============================] - 0s 95us/sample - loss: 0.0220 - val_loss: 0.0170\n",
      "Epoch 51/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0217 - val_loss: 0.0167\n",
      "Epoch 52/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0214 - val_loss: 0.0164\n",
      "Epoch 53/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0211 - val_loss: 0.0161\n",
      "Epoch 54/1000\n",
      "535/535 [==============================] - 0s 92us/sample - loss: 0.0209 - val_loss: 0.0159\n",
      "Epoch 55/1000\n",
      "535/535 [==============================] - 0s 82us/sample - loss: 0.0207 - val_loss: 0.0157\n",
      "Epoch 56/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0205 - val_loss: 0.0155\n",
      "Epoch 57/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0204 - val_loss: 0.0154\n",
      "Epoch 58/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0203 - val_loss: 0.0152\n",
      "Epoch 59/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0202 - val_loss: 0.0151\n",
      "Epoch 60/1000\n",
      "535/535 [==============================] - 0s 78us/sample - loss: 0.0201 - val_loss: 0.0150\n",
      "Epoch 61/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0200 - val_loss: 0.0150\n",
      "Epoch 62/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0199 - val_loss: 0.0150\n",
      "Epoch 63/1000\n",
      "535/535 [==============================] - 0s 49us/sample - loss: 0.0198 - val_loss: 0.0149\n",
      "Epoch 64/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0197 - val_loss: 0.0148\n",
      "Epoch 65/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0196 - val_loss: 0.0148\n",
      "Epoch 66/1000\n",
      "535/535 [==============================] - 0s 79us/sample - loss: 0.0196 - val_loss: 0.0147\n",
      "Epoch 67/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0195 - val_loss: 0.0147\n",
      "Epoch 68/1000\n",
      "535/535 [==============================] - 0s 125us/sample - loss: 0.0195 - val_loss: 0.0146\n",
      "Epoch 69/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0194 - val_loss: 0.0146\n",
      "Epoch 70/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0194 - val_loss: 0.0145\n",
      "Epoch 71/1000\n",
      "535/535 [==============================] - 0s 77us/sample - loss: 0.0194 - val_loss: 0.0145\n",
      "Epoch 72/1000\n",
      "535/535 [==============================] - 0s 105us/sample - loss: 0.0193 - val_loss: 0.0145\n",
      "Epoch 73/1000\n",
      "535/535 [==============================] - 0s 77us/sample - loss: 0.0193 - val_loss: 0.0145\n",
      "Epoch 74/1000\n",
      "535/535 [==============================] - 0s 103us/sample - loss: 0.0193 - val_loss: 0.0145\n",
      "Epoch 75/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0193 - val_loss: 0.0144\n",
      "Epoch 76/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0193 - val_loss: 0.0144\n",
      "Epoch 77/1000\n",
      "535/535 [==============================] - 0s 93us/sample - loss: 0.0193 - val_loss: 0.0145\n",
      "Epoch 78/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 79/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 80/1000\n",
      "535/535 [==============================] - 0s 101us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 81/1000\n",
      "535/535 [==============================] - 0s 75us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 82/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 83/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 84/1000\n",
      "535/535 [==============================] - 0s 90us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 85/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 86/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 87/1000\n",
      "535/535 [==============================] - 0s 90us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 88/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 89/1000\n",
      "535/535 [==============================] - 0s 75us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 90/1000\n",
      "535/535 [==============================] - 0s 90us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 91/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0192 - val_loss: 0.0146\n",
      "Epoch 92/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 93/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 94/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0192 - val_loss: 0.0143\n",
      "Epoch 95/1000\n",
      "535/535 [==============================] - 0s 77us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 96/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 97/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0192 - val_loss: 0.0145\n",
      "Epoch 98/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 99/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0192 - val_loss: 0.0146\n",
      "Epoch 100/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0192 - val_loss: 0.0146\n",
      "Epoch 101/1000\n",
      "535/535 [==============================] - 0s 86us/sample - loss: 0.0192 - val_loss: 0.0146\n",
      "Epoch 102/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 103/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 104/1000\n",
      "535/535 [==============================] - 0s 50us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 105/1000\n",
      "535/535 [==============================] - 0s 159us/sample - loss: 0.0192 - val_loss: 0.0144\n",
      "Epoch 106/1000\n",
      "535/535 [==============================] - 0s 95us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 107/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 108/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 109/1000\n",
      "535/535 [==============================] - 0s 52us/sample - loss: 0.0191 - val_loss: 0.0146\n",
      "Epoch 110/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 111/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 112/1000\n",
      "535/535 [==============================] - 0s 78us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 113/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 114/1000\n",
      "535/535 [==============================] - 0s 52us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 115/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 116/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 117/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 118/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 119/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 120/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 121/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 122/1000\n",
      "535/535 [==============================] - 0s 52us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 123/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 124/1000\n",
      "535/535 [==============================] - 0s 45us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 125/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 126/1000\n",
      "535/535 [==============================] - 0s 50us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 127/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 128/1000\n",
      "535/535 [==============================] - 0s 58us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 129/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 130/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 131/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 132/1000\n",
      "535/535 [==============================] - 0s 103us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 133/1000\n",
      "535/535 [==============================] - 0s 114us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 134/1000\n",
      "535/535 [==============================] - 0s 78us/sample - loss: 0.0191 - val_loss: 0.0144\n",
      "Epoch 135/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 136/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 137/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 138/1000\n",
      "535/535 [==============================] - 0s 120us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 139/1000\n",
      "535/535 [==============================] - 0s 86us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 140/1000\n",
      "535/535 [==============================] - 0s 77us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 141/1000\n",
      "535/535 [==============================] - 0s 97us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 142/1000\n",
      "535/535 [==============================] - 0s 157us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 143/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 144/1000\n",
      "535/535 [==============================] - 0s 93us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 145/1000\n",
      "535/535 [==============================] - 0s 80us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 146/1000\n",
      "535/535 [==============================] - 0s 153us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 147/1000\n",
      "535/535 [==============================] - 0s 125us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 148/1000\n",
      "535/535 [==============================] - 0s 151us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 149/1000\n",
      "535/535 [==============================] - 0s 114us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 150/1000\n",
      "535/535 [==============================] - 0s 75us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 151/1000\n",
      "535/535 [==============================] - 0s 120us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 152/1000\n",
      "535/535 [==============================] - 0s 153us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 153/1000\n",
      "535/535 [==============================] - 0s 108us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 154/1000\n",
      "535/535 [==============================] - 0s 90us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 155/1000\n",
      "535/535 [==============================] - 0s 112us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 156/1000\n",
      "535/535 [==============================] - 0s 129us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 157/1000\n",
      "535/535 [==============================] - 0s 108us/sample - loss: 0.0191 - val_loss: 0.0145\n",
      "Epoch 158/1000\n",
      "535/535 [==============================] - 0s 112us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 159/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 160/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 161/1000\n",
      "535/535 [==============================] - 0s 80us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 162/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 163/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 164/1000\n",
      "535/535 [==============================] - 0s 92us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 165/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 166/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 167/1000\n",
      "535/535 [==============================] - 0s 80us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 168/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 169/1000\n",
      "535/535 [==============================] - 0s 105us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 170/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 171/1000\n",
      "535/535 [==============================] - 0s 78us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 172/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 173/1000\n",
      "535/535 [==============================] - 0s 92us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 174/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 175/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 176/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 177/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 178/1000\n",
      "535/535 [==============================] - 0s 86us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 179/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 180/1000\n",
      "535/535 [==============================] - 0s 84us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 181/1000\n",
      "535/535 [==============================] - 0s 90us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 182/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 183/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 184/1000\n",
      "535/535 [==============================] - 0s 140us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 185/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 186/1000\n",
      "535/535 [==============================] - 0s 92us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 187/1000\n",
      "535/535 [==============================] - 0s 153us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 188/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 189/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 190/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 191/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 192/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 193/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 194/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 195/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 196/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 197/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 198/1000\n",
      "535/535 [==============================] - 0s 64us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 199/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 200/1000\n",
      "535/535 [==============================] - 0s 73us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 201/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 202/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 203/1000\n",
      "535/535 [==============================] - 0s 82us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 204/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 205/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 206/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 207/1000\n",
      "535/535 [==============================] - 0s 71us/sample - loss: 0.0190 - val_loss: 0.0145\n",
      "Epoch 208/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 209/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 210/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 211/1000\n",
      "535/535 [==============================] - 0s 88us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 212/1000\n",
      "535/535 [==============================] - 0s 65us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 213/1000\n",
      "535/535 [==============================] - 0s 67us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 214/1000\n",
      "535/535 [==============================] - 0s 613us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 215/1000\n",
      "535/535 [==============================] - 0s 204us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 216/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 217/1000\n",
      "535/535 [==============================] - 0s 50us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 218/1000\n",
      "535/535 [==============================] - 0s 181us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 219/1000\n",
      "535/535 [==============================] - 0s 164us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 220/1000\n",
      "535/535 [==============================] - 0s 99us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 221/1000\n",
      "535/535 [==============================] - 0s 54us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 222/1000\n",
      "535/535 [==============================] - 0s 49us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 223/1000\n",
      "535/535 [==============================] - 0s 52us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 224/1000\n",
      "535/535 [==============================] - 0s 60us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 225/1000\n",
      "535/535 [==============================] - 0s 69us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 226/1000\n",
      "535/535 [==============================] - 0s 49us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 227/1000\n",
      "535/535 [==============================] - 0s 47us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 228/1000\n",
      "535/535 [==============================] - 0s 56us/sample - loss: 0.0190 - val_loss: 0.0146\n",
      "Epoch 229/1000\n",
      "535/535 [==============================] - 0s 62us/sample - loss: 0.0190 - val_loss: 0.0147\n",
      "Epoch 00229: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e2c61a9c8>"
      ]
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "source": [
    "# 多對多模型 (輸入與輸出相同長度)\n",
    "將return_sequences 設為True ，再用TimeDistributed(Dense(1)) 將輸出調整為(5,1)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildManyToManyModel(shape):\n",
    "#   model = Sequential()\n",
    "#   model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
    "#   # output shape: (5, 1)\n",
    "#   model.add(TimeDistributed(Dense(1)))\n",
    "#   model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "#   model.summary()\n",
    "#   return model\n",
    "    model = tf.keras.Sequential([\n",
    "            tf.keras.layers.LSTM(units=10, input_length=shape[1], input_dim=shape[2], return_sequences=True),\n",
    "            tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1)),\n",
    "            ]) \n",
    "    model.compile(loss='mean_squared_error', \n",
    "            optimizer=tf.keras.optimizers.Adam(),\n",
    "            # metrics=[tf.keras.metrics.MeanAbsoluteError()]\n",
    "            ) \n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ple - loss: 0.0189 - val_loss: 0.0175\n",
      "Epoch 540/1000\n",
      "531/531 [==============================] - 0s 98us/sample - loss: 0.0189 - val_loss: 0.0174\n",
      "Epoch 541/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0189 - val_loss: 0.0174\n",
      "Epoch 542/1000\n",
      "531/531 [==============================] - 0s 105us/sample - loss: 0.0189 - val_loss: 0.0176\n",
      "Epoch 543/1000\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.0189 - val_loss: 0.0177\n",
      "Epoch 544/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0188 - val_loss: 0.0177\n",
      "Epoch 545/1000\n",
      "531/531 [==============================] - 0s 113us/sample - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 546/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0188 - val_loss: 0.0175\n",
      "Epoch 547/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0188 - val_loss: 0.0175\n",
      "Epoch 548/1000\n",
      "531/531 [==============================] - 0s 98us/sample - loss: 0.0188 - val_loss: 0.0174\n",
      "Epoch 549/1000\n",
      "531/531 [==============================] - 0s 92us/sample - loss: 0.0188 - val_loss: 0.0171\n",
      "Epoch 550/1000\n",
      "531/531 [==============================] - 0s 115us/sample - loss: 0.0188 - val_loss: 0.0171\n",
      "Epoch 551/1000\n",
      "531/531 [==============================] - 0s 126us/sample - loss: 0.0188 - val_loss: 0.0173\n",
      "Epoch 552/1000\n",
      "531/531 [==============================] - 0s 126us/sample - loss: 0.0188 - val_loss: 0.0174\n",
      "Epoch 553/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0188 - val_loss: 0.0173\n",
      "Epoch 554/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0187 - val_loss: 0.0172\n",
      "Epoch 555/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0188 - val_loss: 0.0173\n",
      "Epoch 556/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0187 - val_loss: 0.0175\n",
      "Epoch 557/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0188 - val_loss: 0.0176\n",
      "Epoch 558/1000\n",
      "531/531 [==============================] - 0s 124us/sample - loss: 0.0187 - val_loss: 0.0174\n",
      "Epoch 559/1000\n",
      "531/531 [==============================] - 0s 126us/sample - loss: 0.0187 - val_loss: 0.0173\n",
      "Epoch 560/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0188 - val_loss: 0.0172\n",
      "Epoch 561/1000\n",
      "531/531 [==============================] - 0s 96us/sample - loss: 0.0188 - val_loss: 0.0172\n",
      "Epoch 562/1000\n",
      "531/531 [==============================] - 0s 122us/sample - loss: 0.0187 - val_loss: 0.0171\n",
      "Epoch 563/1000\n",
      "531/531 [==============================] - 0s 147us/sample - loss: 0.0187 - val_loss: 0.0170\n",
      "Epoch 564/1000\n",
      "531/531 [==============================] - 0s 105us/sample - loss: 0.0187 - val_loss: 0.0169\n",
      "Epoch 565/1000\n",
      "531/531 [==============================] - 0s 105us/sample - loss: 0.0187 - val_loss: 0.0170\n",
      "Epoch 566/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0187 - val_loss: 0.0169\n",
      "Epoch 567/1000\n",
      "531/531 [==============================] - 0s 89us/sample - loss: 0.0187 - val_loss: 0.0169\n",
      "Epoch 568/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0186 - val_loss: 0.0172\n",
      "Epoch 569/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0187 - val_loss: 0.0173\n",
      "Epoch 570/1000\n",
      "531/531 [==============================] - 0s 169us/sample - loss: 0.0187 - val_loss: 0.0172\n",
      "Epoch 571/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0186 - val_loss: 0.0171\n",
      "Epoch 572/1000\n",
      "531/531 [==============================] - 0s 109us/sample - loss: 0.0187 - val_loss: 0.0169\n",
      "Epoch 573/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0186 - val_loss: 0.0169\n",
      "Epoch 574/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0186 - val_loss: 0.0169\n",
      "Epoch 575/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0186 - val_loss: 0.0171\n",
      "Epoch 576/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0186 - val_loss: 0.0172\n",
      "Epoch 577/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0187 - val_loss: 0.0172\n",
      "Epoch 578/1000\n",
      "531/531 [==============================] - 0s 121us/sample - loss: 0.0187 - val_loss: 0.0171\n",
      "Epoch 579/1000\n",
      "531/531 [==============================] - 0s 113us/sample - loss: 0.0187 - val_loss: 0.0171\n",
      "Epoch 580/1000\n",
      "531/531 [==============================] - 0s 255us/sample - loss: 0.0186 - val_loss: 0.0172\n",
      "Epoch 581/1000\n",
      "531/531 [==============================] - 0s 156us/sample - loss: 0.0186 - val_loss: 0.0171\n",
      "Epoch 582/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0186 - val_loss: 0.0169\n",
      "Epoch 583/1000\n",
      "531/531 [==============================] - 0s 117us/sample - loss: 0.0186 - val_loss: 0.0169\n",
      "Epoch 584/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0186 - val_loss: 0.0170\n",
      "Epoch 585/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0186 - val_loss: 0.0170\n",
      "Epoch 586/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0186 - val_loss: 0.0170\n",
      "Epoch 587/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 588/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0186 - val_loss: 0.0170\n",
      "Epoch 589/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0186 - val_loss: 0.0170\n",
      "Epoch 590/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0185 - val_loss: 0.0168\n",
      "Epoch 591/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0186 - val_loss: 0.0167\n",
      "Epoch 592/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 593/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 594/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0185 - val_loss: 0.0168\n",
      "Epoch 595/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 596/1000\n",
      "531/531 [==============================] - 0s 104us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 597/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 598/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 599/1000\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.0185 - val_loss: 0.0168\n",
      "Epoch 600/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 601/1000\n",
      "531/531 [==============================] - 0s 158us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 602/1000\n",
      "531/531 [==============================] - 0s 166us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 603/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 604/1000\n",
      "531/531 [==============================] - 0s 104us/sample - loss: 0.0185 - val_loss: 0.0168\n",
      "Epoch 605/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 606/1000\n",
      "531/531 [==============================] - 0s 90us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 607/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 608/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0185 - val_loss: 0.0166\n",
      "Epoch 609/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 610/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0185 - val_loss: 0.0169\n",
      "Epoch 611/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0184 - val_loss: 0.0168\n",
      "Epoch 612/1000\n",
      "531/531 [==============================] - 0s 90us/sample - loss: 0.0184 - val_loss: 0.0168\n",
      "Epoch 613/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0185 - val_loss: 0.0167\n",
      "Epoch 614/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0185 - val_loss: 0.0166\n",
      "Epoch 615/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 616/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0184 - val_loss: 0.0167\n",
      "Epoch 617/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 618/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0184 - val_loss: 0.0165\n",
      "Epoch 619/1000\n",
      "531/531 [==============================] - 0s 87us/sample - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 620/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0184 - val_loss: 0.0167\n",
      "Epoch 621/1000\n",
      "531/531 [==============================] - 0s 100us/sample - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 622/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 623/1000\n",
      "531/531 [==============================] - 0s 96us/sample - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 624/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0184 - val_loss: 0.0168\n",
      "Epoch 625/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0184 - val_loss: 0.0167\n",
      "Epoch 626/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0184 - val_loss: 0.0167\n",
      "Epoch 627/1000\n",
      "531/531 [==============================] - 0s 88us/sample - loss: 0.0183 - val_loss: 0.0168\n",
      "Epoch 628/1000\n",
      "531/531 [==============================] - 0s 107us/sample - loss: 0.0183 - val_loss: 0.0167\n",
      "Epoch 629/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0183 - val_loss: 0.0168\n",
      "Epoch 630/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0184 - val_loss: 0.0168\n",
      "Epoch 631/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 632/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 633/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 634/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 635/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 636/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 637/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 638/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 639/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 640/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 641/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0183 - val_loss: 0.0164\n",
      "Epoch 642/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 643/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 644/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 645/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0183 - val_loss: 0.0167\n",
      "Epoch 646/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 647/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0182 - val_loss: 0.0166\n",
      "Epoch 648/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 649/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0183 - val_loss: 0.0165\n",
      "Epoch 650/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 651/1000\n",
      "531/531 [==============================] - 0s 98us/sample - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 652/1000\n",
      "531/531 [==============================] - 0s 109us/sample - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 653/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 654/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 655/1000\n",
      "531/531 [==============================] - 0s 90us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 656/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 657/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0182 - val_loss: 0.0166\n",
      "Epoch 658/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0182 - val_loss: 0.0167\n",
      "Epoch 659/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0183 - val_loss: 0.0167\n",
      "Epoch 660/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0183 - val_loss: 0.0166\n",
      "Epoch 661/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 662/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 663/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 664/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 665/1000\n",
      "531/531 [==============================] - 0s 124us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 666/1000\n",
      "531/531 [==============================] - 0s 75us/sample - loss: 0.0182 - val_loss: 0.0164\n",
      "Epoch 667/1000\n",
      "531/531 [==============================] - 0s 252us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 668/1000\n",
      "531/531 [==============================] - 0s 105us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 669/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 670/1000\n",
      "531/531 [==============================] - 0s 89us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 671/1000\n",
      "531/531 [==============================] - 0s 81us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 672/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 673/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 674/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 675/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 676/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 677/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 678/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0181 - val_loss: 0.0162\n",
      "Epoch 679/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 680/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0182 - val_loss: 0.0165\n",
      "Epoch 681/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0182 - val_loss: 0.0163\n",
      "Epoch 682/1000\n",
      "531/531 [==============================] - 0s 102us/sample - loss: 0.0182 - val_loss: 0.0162\n",
      "Epoch 683/1000\n",
      "531/531 [==============================] - 0s 98us/sample - loss: 0.0181 - val_loss: 0.0162\n",
      "Epoch 684/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 685/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0181 - val_loss: 0.0166\n",
      "Epoch 686/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 687/1000\n",
      "531/531 [==============================] - 0s 85us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 688/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 689/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 690/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 691/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 692/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0181 - val_loss: 0.0163\n",
      "Epoch 693/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 694/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0181 - val_loss: 0.0164\n",
      "Epoch 695/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 696/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 697/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 698/1000\n",
      "531/531 [==============================] - 0s 66us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 699/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 700/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 701/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 702/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 703/1000\n",
      "531/531 [==============================] - 0s 68us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 704/1000\n",
      "531/531 [==============================] - 0s 94us/sample - loss: 0.0180 - val_loss: 0.0163\n",
      "Epoch 705/1000\n",
      "531/531 [==============================] - 0s 160us/sample - loss: 0.0180 - val_loss: 0.0163\n",
      "Epoch 706/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0180 - val_loss: 0.0163\n",
      "Epoch 707/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0180 - val_loss: 0.0163\n",
      "Epoch 708/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 709/1000\n",
      "531/531 [==============================] - 0s 62us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 710/1000\n",
      "531/531 [==============================] - 0s 77us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 711/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 712/1000\n",
      "531/531 [==============================] - 0s 83us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 713/1000\n",
      "531/531 [==============================] - 0s 64us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 714/1000\n",
      "531/531 [==============================] - 0s 98us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 715/1000\n",
      "531/531 [==============================] - 0s 344us/sample - loss: 0.0179 - val_loss: 0.0159\n",
      "Epoch 716/1000\n",
      "531/531 [==============================] - 0s 166us/sample - loss: 0.0180 - val_loss: 0.0159\n",
      "Epoch 717/1000\n",
      "531/531 [==============================] - 0s 90us/sample - loss: 0.0180 - val_loss: 0.0159\n",
      "Epoch 718/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0180 - val_loss: 0.0162\n",
      "Epoch 719/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 720/1000\n",
      "531/531 [==============================] - 0s 89us/sample - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 721/1000\n",
      "531/531 [==============================] - 0s 72us/sample - loss: 0.0180 - val_loss: 0.0159\n",
      "Epoch 722/1000\n",
      "531/531 [==============================] - 0s 70us/sample - loss: 0.0180 - val_loss: 0.0159\n",
      "Epoch 723/1000\n",
      "531/531 [==============================] - 0s 89us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 724/1000\n",
      "531/531 [==============================] - 0s 79us/sample - loss: 0.0180 - val_loss: 0.0161\n",
      "Epoch 725/1000\n",
      "531/531 [==============================] - 0s 73us/sample - loss: 0.0179 - val_loss: 0.0160\n",
      "Epoch 00725: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20e31652188>"
      ]
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=10, verbose=0, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}